{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from utils import load_pickle\n",
    "\n",
    "import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting room type\n",
    "\n",
    "This notebook can be used to train a GNN to predict room type from zoning type. Then save graph_pred files for the test set that contain the predicted room_types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zoning_attribute(graph: nx.Graph):\n",
    "    room_type = nx.get_node_attributes(graph, 'room_type')\n",
    "\n",
    "    room_names = {node: constants.ROOM_NAMES[value] for node, value in room_type.items()}\n",
    "\n",
    "    inv_room_mapping = {val: key for key, val in constants.ROOM_MAPPING.items()}\n",
    "\n",
    "    room_names = {node: inv_room_mapping[value] for node, value in room_names.items()}\n",
    "\n",
    "    zoning = {key: constants.ZONING_MAPPING[value] for key, value in room_names.items()}\n",
    "\n",
    "    zoning_index = {key: constants.ZONING_NAMES.index(value) for key, value in zoning.items()}\n",
    "\n",
    "    nx.set_node_attributes(graph, zoning_index, 'zoning_type')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def one_hot_encode(value, num_classes):\n",
    "    return torch.eye(num_classes)[value]\n",
    "\n",
    "\n",
    "NUM_ROOM_TYPES = 9\n",
    "NUM_ZONING_TYPES = 4\n",
    "\n",
    "# def one_hot_encode_types(graph: nx.Graph):\n",
    "#     for node in graph.nodes:\n",
    "#         graph.nodes[node]['room_type'] = one_hot_encode(graph.nodes[node]['room_type'], NUM_ROOM_TYPES)\n",
    "#         graph.nodes[node]['zoning_type'] = one_hot_encode(graph.nodes[node]['zoning_type'], NUM_ZONING_TYPES)\n",
    "\n",
    "CONNECTIVITIES = [\"door\", \"entrance\", \"passage\"]\n",
    "\n",
    "class GraphZoningRoomTypeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Graph Dataset. Collects NetworkX graph from a pre-defined folder and\n",
    "    transforms them to Pytorch Geometric (pyg.data.Data()) instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, split=\"train\"):\n",
    "        # self.graph_in_path = os.path.join(path, 'graph_in')\n",
    "        self.graph_out_path = os.path.join(path, 'graph_out')\n",
    "\n",
    "        # include graph transformations if you like\n",
    "        # self.graph_transform = graph_transform\n",
    "\n",
    "        all_files = os.listdir(self.graph_out_path)\n",
    "\n",
    "        self.train_files, self.val_files = train_test_split(all_files, test_size=0.05, random_state=42)\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.files = self.train_files\n",
    "        elif split == \"val\":\n",
    "            self.files = self.val_files\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file_name = self.files[index]\n",
    "\n",
    "        # get access graph (name is index)\n",
    "        graph_nx = load_pickle(os.path.join(self.graph_out_path, file_name))\n",
    "\n",
    "        add_zoning_attribute(graph_nx)\n",
    "\n",
    "        for edge in graph_nx.edges:\n",
    "            graph_nx.edges[edge]['connectivity'] = CONNECTIVITIES.index(graph_nx.edges[edge]['connectivity'])\n",
    "\n",
    "        # Remove attributes geometry, centroid\n",
    "        for node in graph_nx.nodes:\n",
    "            graph_nx.nodes[node].pop('geometry', None)\n",
    "            graph_nx.nodes[node].pop('centroid', None)\n",
    "\n",
    "        # graph_nx.graph[]\n",
    "\n",
    "        # transform networkx graph to pytorch geometric graph\n",
    "        graph_pyg = pyg.utils.from_networkx(graph_nx)\n",
    "\n",
    "        # transform graph if you like\n",
    "        graph_pyg = self.graph_transform(graph_pyg)\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    @staticmethod\n",
    "    def graph_transform(graph_pyg):\n",
    "        graph_pyg[\"room_type\"] = one_hot_encode(graph_pyg[\"room_type\"], NUM_ROOM_TYPES)\n",
    "        graph_pyg[\"zoning_type\"] = one_hot_encode(graph_pyg[\"zoning_type\"], NUM_ZONING_TYPES)\n",
    "\n",
    "        graph_pyg[\"connectivity\"] = one_hot_encode(graph_pyg[\"connectivity\"], len(CONNECTIVITIES))\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphZoningTypeTestSet(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Graph Dataset. Collects NetworkX graph from a pre-defined folder and\n",
    "    transforms them to Pytorch Geometric (pyg.data.Data()) instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.graph_path = os.path.join(path, 'graph_in')\n",
    "\n",
    "        self.files = os.listdir(self.graph_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file_name = self.files[index]\n",
    "\n",
    "        # get access graph (name is index)\n",
    "        graph_nx = load_pickle(os.path.join(self.graph_path, file_name))\n",
    "\n",
    "        for edge in graph_nx.edges:\n",
    "            graph_nx.edges[edge]['connectivity'] = CONNECTIVITIES.index(graph_nx.edges[edge]['connectivity'])\n",
    "\n",
    "        # transform networkx graph to pytorch geometric graph\n",
    "        graph_pyg = pyg.utils.from_networkx(graph_nx)\n",
    "\n",
    "        # transform graph if you like\n",
    "        graph_pyg = self.graph_transform(graph_pyg)\n",
    "\n",
    "        graph_pyg[\"file_name\"] = file_name\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    @staticmethod\n",
    "    def graph_transform(graph_pyg):\n",
    "        graph_pyg[\"zoning_type\"] = one_hot_encode(graph_pyg[\"zoning_type\"], NUM_ZONING_TYPES)\n",
    "\n",
    "        graph_pyg[\"connectivity\"] = one_hot_encode(graph_pyg[\"connectivity\"], len(CONNECTIVITIES))\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/path/to/modified-swiss-dwellings/modified-swiss-dwellings-v1-train/\"\n",
    "\n",
    "# graph_nx = load_pickle(os.path.join(graph_path, f'{43}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_nx.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = GraphZoningRoomTypeDataset(path, split=\"train\")\n",
    "\n",
    "# A small portion of the training set is reserved for validation:\n",
    "ds_val = GraphZoningRoomTypeDataset(path, split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, num_features=NUM_ZONING_TYPES, num_edge_features=len(CONNECTIVITIES), hidden_size=32, target_size=NUM_ROOM_TYPES, num_hidden_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.target_size = target_size\n",
    "\n",
    "        self.convs = nn.ModuleList([GATConv(self.num_features if i == 0 else self.hidden_size, self.hidden_size, edge_dim=self.num_edge_features) for i in range(num_hidden_layers)])\n",
    "\n",
    "        self.hidden_linear = nn.Linear(self.hidden_size + num_features, self.hidden_size)\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.zoning_type, data.edge_index, data.connectivity\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr) # adding edge features here!\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "\n",
    "        # x = self.convs[-1](x, edge_index, edge_attr=edge_attr) # edge features here as well\n",
    "        \n",
    "        x = torch.cat([x, data.zoning_type], dim=-1)\n",
    "\n",
    "        x = self.hidden_linear(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return F.relu(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate_model(model, data_val, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    accuracies = []\n",
    "    ious = []\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Batch size should be 1, because want to evaluate each graph separately\n",
    "        for data in DataLoader(data_val, batch_size=1):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "            pred = torch.argmax(model(data.to(device)), dim=-1)\n",
    "\n",
    "            gt = torch.argmax(data.room_type, dim=-1).to(device)\n",
    "            \n",
    "            # Tensor of shape (batch_size, 1)\n",
    "            acc = (gt == pred).sum(dim=-1) / pred.shape[-1]\n",
    "\n",
    "            pred_counter = Counter(pred.cpu().numpy())\n",
    "            gt_counter = Counter(gt.cpu().numpy())\n",
    "\n",
    "            iou = sum((pred_counter & gt_counter).values()) / sum((pred_counter | gt_counter).values())\n",
    "            ious.append(iou)\n",
    "\n",
    "            accuracies.append(acc.item())\n",
    "            \n",
    "            val_loss = F.cross_entropy(out, data.room_type)\n",
    "\n",
    "            total_loss += val_loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return total_loss, np.mean(accuracies), np.mean(ious)\n",
    "\n",
    "\n",
    "def train(model, data_train, data_val, batch_size, learning_rate, n_epochs=1, device=\"cpu\", save_loss_interval=1, print_interval=1):\n",
    "\n",
    "    NUM_TRAIN = len(data_train)\n",
    "    NUM_VAL = len(data_val)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # test_data = data_test[0]\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for data in loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(data)\n",
    "            \n",
    "            loss = F.cross_entropy(out, data.room_type)\n",
    "            \n",
    "            epoch_loss += loss.item() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % save_loss_interval == 0:\n",
    "            val_loss, val_acc, val_iou = evaluate_model(model, data_val, device=\"cpu\")\n",
    "\n",
    "            val_loss /= NUM_VAL\n",
    "\n",
    "            model = model.to(device)\n",
    "\n",
    "            train_loss = epoch_loss / NUM_TRAIN\n",
    "          \n",
    "            if epoch % print_interval == 0:\n",
    "                print(f\"Epoch: {epoch} Train loss: {train_loss:.3e} Val loss: {val_loss:.3e} Val acc: {val_acc:.3f} Val iou: {val_iou:.3f}\")\n",
    "          \n",
    "            yield {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_iou\": val_iou,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_with_early_stopping(model, ds_train, ds_val, batch_size=32, learning_rate=0.001, n_epochs=100, device=\"cuda\", tolerance=5):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    best_model = (None, 1e5, None)\n",
    "\n",
    "    for loss in train(model, ds_train, ds_val, batch_size=batch_size, learning_rate=learning_rate, n_epochs=n_epochs, device=device):\n",
    "        losses.append(loss)\n",
    "\n",
    "        if loss[\"val_loss\"] < best_model[1]:\n",
    "            best_model = (copy.deepcopy(model), loss[\"val_loss\"], loss)\n",
    "\n",
    "            print(f\"New best model with val loss: {loss['val_loss']:.3e}\")\n",
    "        \n",
    "        if loss[\"epoch\"] - best_model[2][\"epoch\"] > tolerance:\n",
    "            print(f\"Stopping early at epoch {loss['epoch']}\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"losses\": losses,\n",
    "        \"best_model\": best_model[0],\n",
    "        \"best_model_val_loss\": best_model[1],\n",
    "        \"best_model_loss_dict\": best_model[2],\n",
    "        \"last_model\": model,\n",
    "    }\n",
    "\n",
    "# train_with_early_stopping(model, ds_train, ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run hyperparameter search\n",
    "\n",
    "The num_hidden_layers parameter seemed to have the largest effect on accuracy, and thus a search is performed for this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 2 hidden layers\n",
      "Epoch: 0 Train loss: 5.591e-02 Val loss: 1.144e+00 Val acc: 0.708 Val iou: 0.585\n",
      "New best model with val loss: 1.144e+00\n",
      "Epoch: 1 Train loss: 3.369e-02 Val loss: 7.187e-01 Val acc: 0.761 Val iou: 0.646\n",
      "New best model with val loss: 7.187e-01\n",
      "Epoch: 2 Train loss: 2.477e-02 Val loss: 5.653e-01 Val acc: 0.776 Val iou: 0.665\n",
      "New best model with val loss: 5.653e-01\n",
      "Epoch: 3 Train loss: 2.111e-02 Val loss: 5.171e-01 Val acc: 0.786 Val iou: 0.689\n",
      "New best model with val loss: 5.171e-01\n",
      "Epoch: 4 Train loss: 1.931e-02 Val loss: 4.820e-01 Val acc: 0.820 Val iou: 0.725\n",
      "New best model with val loss: 4.820e-01\n",
      "Epoch: 5 Train loss: 1.823e-02 Val loss: 4.652e-01 Val acc: 0.827 Val iou: 0.735\n",
      "New best model with val loss: 4.652e-01\n",
      "Epoch: 6 Train loss: 1.761e-02 Val loss: 4.508e-01 Val acc: 0.827 Val iou: 0.734\n",
      "New best model with val loss: 4.508e-01\n",
      "Epoch: 7 Train loss: 1.706e-02 Val loss: 4.380e-01 Val acc: 0.849 Val iou: 0.776\n",
      "New best model with val loss: 4.380e-01\n",
      "Epoch: 8 Train loss: 1.655e-02 Val loss: 4.250e-01 Val acc: 0.851 Val iou: 0.780\n",
      "New best model with val loss: 4.250e-01\n",
      "Epoch: 9 Train loss: 1.629e-02 Val loss: 4.173e-01 Val acc: 0.850 Val iou: 0.777\n",
      "New best model with val loss: 4.173e-01\n",
      "Epoch: 10 Train loss: 1.593e-02 Val loss: 4.063e-01 Val acc: 0.865 Val iou: 0.808\n",
      "New best model with val loss: 4.063e-01\n",
      "Epoch: 11 Train loss: 1.561e-02 Val loss: 3.992e-01 Val acc: 0.866 Val iou: 0.810\n",
      "New best model with val loss: 3.992e-01\n",
      "Epoch: 12 Train loss: 1.527e-02 Val loss: 3.899e-01 Val acc: 0.865 Val iou: 0.811\n",
      "New best model with val loss: 3.899e-01\n",
      "Epoch: 13 Train loss: 1.512e-02 Val loss: 3.836e-01 Val acc: 0.867 Val iou: 0.812\n",
      "New best model with val loss: 3.836e-01\n",
      "Epoch: 14 Train loss: 1.496e-02 Val loss: 3.796e-01 Val acc: 0.871 Val iou: 0.817\n",
      "New best model with val loss: 3.796e-01\n",
      "Epoch: 15 Train loss: 1.478e-02 Val loss: 3.766e-01 Val acc: 0.870 Val iou: 0.817\n",
      "New best model with val loss: 3.766e-01\n",
      "Epoch: 16 Train loss: 1.456e-02 Val loss: 3.733e-01 Val acc: 0.869 Val iou: 0.818\n",
      "New best model with val loss: 3.733e-01\n",
      "Epoch: 17 Train loss: 1.452e-02 Val loss: 3.715e-01 Val acc: 0.869 Val iou: 0.815\n",
      "New best model with val loss: 3.715e-01\n",
      "Epoch: 18 Train loss: 1.437e-02 Val loss: 3.692e-01 Val acc: 0.872 Val iou: 0.819\n",
      "New best model with val loss: 3.692e-01\n",
      "Epoch: 19 Train loss: 1.428e-02 Val loss: 3.706e-01 Val acc: 0.871 Val iou: 0.818\n",
      "Epoch: 20 Train loss: 1.419e-02 Val loss: 3.704e-01 Val acc: 0.871 Val iou: 0.819\n",
      "Epoch: 21 Train loss: 1.418e-02 Val loss: 3.694e-01 Val acc: 0.870 Val iou: 0.817\n",
      "Epoch: 22 Train loss: 1.406e-02 Val loss: 3.672e-01 Val acc: 0.870 Val iou: 0.817\n",
      "New best model with val loss: 3.672e-01\n",
      "Epoch: 23 Train loss: 1.401e-02 Val loss: 3.657e-01 Val acc: 0.869 Val iou: 0.817\n",
      "New best model with val loss: 3.657e-01\n",
      "Epoch: 24 Train loss: 1.388e-02 Val loss: 3.663e-01 Val acc: 0.871 Val iou: 0.817\n",
      "Epoch: 25 Train loss: 1.383e-02 Val loss: 3.646e-01 Val acc: 0.870 Val iou: 0.816\n",
      "New best model with val loss: 3.646e-01\n",
      "Epoch: 26 Train loss: 1.382e-02 Val loss: 3.648e-01 Val acc: 0.871 Val iou: 0.814\n",
      "Epoch: 27 Train loss: 1.373e-02 Val loss: 3.647e-01 Val acc: 0.872 Val iou: 0.819\n",
      "Epoch: 28 Train loss: 1.372e-02 Val loss: 3.631e-01 Val acc: 0.872 Val iou: 0.816\n",
      "New best model with val loss: 3.631e-01\n",
      "Epoch: 29 Train loss: 1.370e-02 Val loss: 3.626e-01 Val acc: 0.872 Val iou: 0.821\n",
      "New best model with val loss: 3.626e-01\n",
      "Epoch: 30 Train loss: 1.364e-02 Val loss: 3.612e-01 Val acc: 0.870 Val iou: 0.816\n",
      "New best model with val loss: 3.612e-01\n",
      "Epoch: 31 Train loss: 1.363e-02 Val loss: 3.612e-01 Val acc: 0.872 Val iou: 0.817\n",
      "New best model with val loss: 3.612e-01\n",
      "Epoch: 32 Train loss: 1.354e-02 Val loss: 3.597e-01 Val acc: 0.870 Val iou: 0.815\n",
      "New best model with val loss: 3.597e-01\n",
      "Epoch: 33 Train loss: 1.348e-02 Val loss: 3.596e-01 Val acc: 0.873 Val iou: 0.817\n",
      "New best model with val loss: 3.596e-01\n",
      "Epoch: 34 Train loss: 1.340e-02 Val loss: 3.602e-01 Val acc: 0.871 Val iou: 0.816\n",
      "Epoch: 35 Train loss: 1.351e-02 Val loss: 3.597e-01 Val acc: 0.873 Val iou: 0.818\n",
      "Epoch: 36 Train loss: 1.340e-02 Val loss: 3.593e-01 Val acc: 0.874 Val iou: 0.818\n",
      "New best model with val loss: 3.593e-01\n",
      "Epoch: 37 Train loss: 1.342e-02 Val loss: 3.601e-01 Val acc: 0.874 Val iou: 0.821\n",
      "Epoch: 38 Train loss: 1.341e-02 Val loss: 3.576e-01 Val acc: 0.873 Val iou: 0.818\n",
      "New best model with val loss: 3.576e-01\n",
      "Epoch: 39 Train loss: 1.327e-02 Val loss: 3.581e-01 Val acc: 0.874 Val iou: 0.821\n",
      "Epoch: 40 Train loss: 1.331e-02 Val loss: 3.581e-01 Val acc: 0.873 Val iou: 0.817\n",
      "Epoch: 41 Train loss: 1.329e-02 Val loss: 3.572e-01 Val acc: 0.873 Val iou: 0.818\n",
      "New best model with val loss: 3.572e-01\n",
      "Epoch: 42 Train loss: 1.326e-02 Val loss: 3.568e-01 Val acc: 0.873 Val iou: 0.816\n",
      "New best model with val loss: 3.568e-01\n",
      "Epoch: 43 Train loss: 1.322e-02 Val loss: 3.589e-01 Val acc: 0.874 Val iou: 0.821\n",
      "Epoch: 44 Train loss: 1.318e-02 Val loss: 3.564e-01 Val acc: 0.873 Val iou: 0.816\n",
      "New best model with val loss: 3.564e-01\n",
      "Epoch: 45 Train loss: 1.314e-02 Val loss: 3.566e-01 Val acc: 0.873 Val iou: 0.817\n",
      "Epoch: 46 Train loss: 1.320e-02 Val loss: 3.549e-01 Val acc: 0.874 Val iou: 0.817\n",
      "New best model with val loss: 3.549e-01\n",
      "Epoch: 47 Train loss: 1.315e-02 Val loss: 3.579e-01 Val acc: 0.874 Val iou: 0.819\n",
      "Epoch: 48 Train loss: 1.308e-02 Val loss: 3.548e-01 Val acc: 0.873 Val iou: 0.817\n",
      "New best model with val loss: 3.548e-01\n",
      "Epoch: 49 Train loss: 1.309e-02 Val loss: 3.522e-01 Val acc: 0.874 Val iou: 0.817\n",
      "New best model with val loss: 3.522e-01\n",
      "Epoch: 50 Train loss: 1.303e-02 Val loss: 3.541e-01 Val acc: 0.872 Val iou: 0.816\n",
      "Epoch: 51 Train loss: 1.302e-02 Val loss: 3.518e-01 Val acc: 0.874 Val iou: 0.818\n",
      "New best model with val loss: 3.518e-01\n",
      "Epoch: 52 Train loss: 1.300e-02 Val loss: 3.524e-01 Val acc: 0.873 Val iou: 0.821\n",
      "Epoch: 53 Train loss: 1.298e-02 Val loss: 3.540e-01 Val acc: 0.873 Val iou: 0.820\n",
      "Epoch: 54 Train loss: 1.300e-02 Val loss: 3.533e-01 Val acc: 0.873 Val iou: 0.819\n",
      "Epoch: 55 Train loss: 1.295e-02 Val loss: 3.527e-01 Val acc: 0.874 Val iou: 0.817\n",
      "Epoch: 56 Train loss: 1.294e-02 Val loss: 3.533e-01 Val acc: 0.874 Val iou: 0.816\n",
      "Epoch: 57 Train loss: 1.293e-02 Val loss: 3.520e-01 Val acc: 0.872 Val iou: 0.817\n",
      "Stopping early at epoch 57\n",
      "Training model with 3 hidden layers\n",
      "Epoch: 0 Train loss: 5.774e-02 Val loss: 1.134e+00 Val acc: 0.729 Val iou: 0.609\n",
      "New best model with val loss: 1.134e+00\n",
      "Epoch: 1 Train loss: 3.186e-02 Val loss: 6.230e-01 Val acc: 0.785 Val iou: 0.667\n",
      "New best model with val loss: 6.230e-01\n",
      "Epoch: 2 Train loss: 2.257e-02 Val loss: 5.219e-01 Val acc: 0.788 Val iou: 0.668\n",
      "New best model with val loss: 5.219e-01\n",
      "Epoch: 3 Train loss: 1.961e-02 Val loss: 4.886e-01 Val acc: 0.790 Val iou: 0.672\n",
      "New best model with val loss: 4.886e-01\n",
      "Epoch: 4 Train loss: 1.834e-02 Val loss: 4.663e-01 Val acc: 0.791 Val iou: 0.672\n",
      "New best model with val loss: 4.663e-01\n",
      "Epoch: 5 Train loss: 1.741e-02 Val loss: 4.421e-01 Val acc: 0.792 Val iou: 0.674\n",
      "New best model with val loss: 4.421e-01\n",
      "Epoch: 6 Train loss: 1.664e-02 Val loss: 4.206e-01 Val acc: 0.828 Val iou: 0.732\n",
      "New best model with val loss: 4.206e-01\n",
      "Epoch: 7 Train loss: 1.609e-02 Val loss: 4.070e-01 Val acc: 0.830 Val iou: 0.734\n",
      "New best model with val loss: 4.070e-01\n",
      "Epoch: 8 Train loss: 1.566e-02 Val loss: 3.970e-01 Val acc: 0.832 Val iou: 0.737\n",
      "New best model with val loss: 3.970e-01\n",
      "Epoch: 9 Train loss: 1.518e-02 Val loss: 3.901e-01 Val acc: 0.834 Val iou: 0.739\n",
      "New best model with val loss: 3.901e-01\n",
      "Epoch: 10 Train loss: 1.481e-02 Val loss: 3.806e-01 Val acc: 0.839 Val iou: 0.749\n",
      "New best model with val loss: 3.806e-01\n",
      "Epoch: 11 Train loss: 1.450e-02 Val loss: 3.757e-01 Val acc: 0.841 Val iou: 0.749\n",
      "New best model with val loss: 3.757e-01\n",
      "Epoch: 12 Train loss: 1.432e-02 Val loss: 3.726e-01 Val acc: 0.841 Val iou: 0.749\n",
      "New best model with val loss: 3.726e-01\n",
      "Epoch: 13 Train loss: 1.416e-02 Val loss: 3.716e-01 Val acc: 0.841 Val iou: 0.750\n",
      "New best model with val loss: 3.716e-01\n",
      "Epoch: 14 Train loss: 1.404e-02 Val loss: 3.686e-01 Val acc: 0.842 Val iou: 0.750\n",
      "New best model with val loss: 3.686e-01\n",
      "Epoch: 15 Train loss: 1.390e-02 Val loss: 3.662e-01 Val acc: 0.843 Val iou: 0.752\n",
      "New best model with val loss: 3.662e-01\n",
      "Epoch: 16 Train loss: 1.380e-02 Val loss: 3.649e-01 Val acc: 0.842 Val iou: 0.751\n",
      "New best model with val loss: 3.649e-01\n",
      "Epoch: 17 Train loss: 1.365e-02 Val loss: 3.638e-01 Val acc: 0.843 Val iou: 0.752\n",
      "New best model with val loss: 3.638e-01\n",
      "Epoch: 18 Train loss: 1.364e-02 Val loss: 3.629e-01 Val acc: 0.843 Val iou: 0.751\n",
      "New best model with val loss: 3.629e-01\n",
      "Epoch: 19 Train loss: 1.348e-02 Val loss: 3.610e-01 Val acc: 0.843 Val iou: 0.753\n",
      "New best model with val loss: 3.610e-01\n",
      "Epoch: 20 Train loss: 1.350e-02 Val loss: 3.613e-01 Val acc: 0.843 Val iou: 0.752\n",
      "Epoch: 21 Train loss: 1.342e-02 Val loss: 3.608e-01 Val acc: 0.844 Val iou: 0.753\n",
      "New best model with val loss: 3.608e-01\n",
      "Epoch: 22 Train loss: 1.333e-02 Val loss: 3.595e-01 Val acc: 0.844 Val iou: 0.753\n",
      "New best model with val loss: 3.595e-01\n",
      "Epoch: 23 Train loss: 1.328e-02 Val loss: 3.581e-01 Val acc: 0.844 Val iou: 0.753\n",
      "New best model with val loss: 3.581e-01\n",
      "Epoch: 24 Train loss: 1.325e-02 Val loss: 3.563e-01 Val acc: 0.844 Val iou: 0.753\n",
      "New best model with val loss: 3.563e-01\n",
      "Epoch: 25 Train loss: 1.320e-02 Val loss: 3.583e-01 Val acc: 0.845 Val iou: 0.754\n",
      "Epoch: 26 Train loss: 1.318e-02 Val loss: 3.564e-01 Val acc: 0.845 Val iou: 0.753\n",
      "Epoch: 27 Train loss: 1.307e-02 Val loss: 3.561e-01 Val acc: 0.844 Val iou: 0.753\n",
      "New best model with val loss: 3.561e-01\n",
      "Epoch: 28 Train loss: 1.305e-02 Val loss: 3.555e-01 Val acc: 0.845 Val iou: 0.753\n",
      "New best model with val loss: 3.555e-01\n",
      "Epoch: 29 Train loss: 1.304e-02 Val loss: 3.550e-01 Val acc: 0.845 Val iou: 0.753\n",
      "New best model with val loss: 3.550e-01\n",
      "Epoch: 30 Train loss: 1.298e-02 Val loss: 3.538e-01 Val acc: 0.844 Val iou: 0.753\n",
      "New best model with val loss: 3.538e-01\n",
      "Epoch: 31 Train loss: 1.301e-02 Val loss: 3.531e-01 Val acc: 0.845 Val iou: 0.754\n",
      "New best model with val loss: 3.531e-01\n",
      "Epoch: 32 Train loss: 1.295e-02 Val loss: 3.530e-01 Val acc: 0.845 Val iou: 0.754\n",
      "New best model with val loss: 3.530e-01\n",
      "Epoch: 33 Train loss: 1.298e-02 Val loss: 3.515e-01 Val acc: 0.843 Val iou: 0.756\n",
      "New best model with val loss: 3.515e-01\n",
      "Epoch: 34 Train loss: 1.286e-02 Val loss: 3.529e-01 Val acc: 0.845 Val iou: 0.754\n",
      "Epoch: 35 Train loss: 1.284e-02 Val loss: 3.510e-01 Val acc: 0.843 Val iou: 0.760\n",
      "New best model with val loss: 3.510e-01\n",
      "Epoch: 36 Train loss: 1.286e-02 Val loss: 3.499e-01 Val acc: 0.842 Val iou: 0.759\n",
      "New best model with val loss: 3.499e-01\n",
      "Epoch: 37 Train loss: 1.283e-02 Val loss: 3.514e-01 Val acc: 0.855 Val iou: 0.790\n",
      "Epoch: 38 Train loss: 1.284e-02 Val loss: 3.493e-01 Val acc: 0.852 Val iou: 0.787\n",
      "New best model with val loss: 3.493e-01\n",
      "Epoch: 39 Train loss: 1.277e-02 Val loss: 3.476e-01 Val acc: 0.852 Val iou: 0.787\n",
      "New best model with val loss: 3.476e-01\n",
      "Epoch: 40 Train loss: 1.272e-02 Val loss: 3.461e-01 Val acc: 0.851 Val iou: 0.784\n",
      "New best model with val loss: 3.461e-01\n",
      "Epoch: 41 Train loss: 1.274e-02 Val loss: 3.459e-01 Val acc: 0.853 Val iou: 0.788\n",
      "New best model with val loss: 3.459e-01\n",
      "Epoch: 42 Train loss: 1.269e-02 Val loss: 3.447e-01 Val acc: 0.852 Val iou: 0.786\n",
      "New best model with val loss: 3.447e-01\n",
      "Epoch: 43 Train loss: 1.264e-02 Val loss: 3.414e-01 Val acc: 0.867 Val iou: 0.806\n",
      "New best model with val loss: 3.414e-01\n",
      "Epoch: 44 Train loss: 1.261e-02 Val loss: 3.399e-01 Val acc: 0.854 Val iou: 0.790\n",
      "New best model with val loss: 3.399e-01\n",
      "Epoch: 45 Train loss: 1.257e-02 Val loss: 3.375e-01 Val acc: 0.868 Val iou: 0.809\n",
      "New best model with val loss: 3.375e-01\n",
      "Epoch: 46 Train loss: 1.260e-02 Val loss: 3.366e-01 Val acc: 0.879 Val iou: 0.824\n",
      "New best model with val loss: 3.366e-01\n",
      "Epoch: 47 Train loss: 1.256e-02 Val loss: 3.360e-01 Val acc: 0.881 Val iou: 0.823\n",
      "New best model with val loss: 3.360e-01\n",
      "Epoch: 48 Train loss: 1.255e-02 Val loss: 3.337e-01 Val acc: 0.887 Val iou: 0.828\n",
      "New best model with val loss: 3.337e-01\n",
      "Epoch: 49 Train loss: 1.253e-02 Val loss: 3.328e-01 Val acc: 0.888 Val iou: 0.829\n",
      "New best model with val loss: 3.328e-01\n",
      "Epoch: 50 Train loss: 1.246e-02 Val loss: 3.294e-01 Val acc: 0.890 Val iou: 0.833\n",
      "New best model with val loss: 3.294e-01\n",
      "Epoch: 51 Train loss: 1.245e-02 Val loss: 3.316e-01 Val acc: 0.886 Val iou: 0.829\n",
      "Epoch: 52 Train loss: 1.242e-02 Val loss: 3.288e-01 Val acc: 0.890 Val iou: 0.834\n",
      "New best model with val loss: 3.288e-01\n",
      "Epoch: 53 Train loss: 1.238e-02 Val loss: 3.235e-01 Val acc: 0.893 Val iou: 0.837\n",
      "New best model with val loss: 3.235e-01\n",
      "Epoch: 54 Train loss: 1.228e-02 Val loss: 3.221e-01 Val acc: 0.896 Val iou: 0.841\n",
      "New best model with val loss: 3.221e-01\n",
      "Epoch: 55 Train loss: 1.230e-02 Val loss: 3.211e-01 Val acc: 0.895 Val iou: 0.839\n",
      "New best model with val loss: 3.211e-01\n",
      "Epoch: 56 Train loss: 1.227e-02 Val loss: 3.199e-01 Val acc: 0.893 Val iou: 0.838\n",
      "New best model with val loss: 3.199e-01\n",
      "Epoch: 57 Train loss: 1.231e-02 Val loss: 3.195e-01 Val acc: 0.896 Val iou: 0.842\n",
      "New best model with val loss: 3.195e-01\n",
      "Epoch: 58 Train loss: 1.221e-02 Val loss: 3.186e-01 Val acc: 0.893 Val iou: 0.839\n",
      "New best model with val loss: 3.186e-01\n",
      "Epoch: 59 Train loss: 1.216e-02 Val loss: 3.142e-01 Val acc: 0.895 Val iou: 0.841\n",
      "New best model with val loss: 3.142e-01\n",
      "Epoch: 60 Train loss: 1.213e-02 Val loss: 3.145e-01 Val acc: 0.897 Val iou: 0.843\n",
      "Epoch: 61 Train loss: 1.204e-02 Val loss: 3.142e-01 Val acc: 0.895 Val iou: 0.841\n",
      "New best model with val loss: 3.142e-01\n",
      "Epoch: 62 Train loss: 1.209e-02 Val loss: 3.113e-01 Val acc: 0.894 Val iou: 0.839\n",
      "New best model with val loss: 3.113e-01\n",
      "Epoch: 63 Train loss: 1.204e-02 Val loss: 3.150e-01 Val acc: 0.895 Val iou: 0.840\n",
      "Epoch: 64 Train loss: 1.198e-02 Val loss: 3.081e-01 Val acc: 0.895 Val iou: 0.841\n",
      "New best model with val loss: 3.081e-01\n",
      "Epoch: 65 Train loss: 1.201e-02 Val loss: 3.098e-01 Val acc: 0.896 Val iou: 0.842\n",
      "Epoch: 66 Train loss: 1.202e-02 Val loss: 3.102e-01 Val acc: 0.895 Val iou: 0.841\n",
      "Epoch: 67 Train loss: 1.199e-02 Val loss: 3.100e-01 Val acc: 0.897 Val iou: 0.843\n",
      "Epoch: 68 Train loss: 1.196e-02 Val loss: 3.069e-01 Val acc: 0.897 Val iou: 0.843\n",
      "New best model with val loss: 3.069e-01\n",
      "Epoch: 69 Train loss: 1.188e-02 Val loss: 3.087e-01 Val acc: 0.896 Val iou: 0.840\n",
      "Epoch: 70 Train loss: 1.192e-02 Val loss: 3.067e-01 Val acc: 0.896 Val iou: 0.841\n",
      "New best model with val loss: 3.067e-01\n",
      "Epoch: 71 Train loss: 1.193e-02 Val loss: 3.074e-01 Val acc: 0.894 Val iou: 0.839\n",
      "Epoch: 72 Train loss: 1.192e-02 Val loss: 3.059e-01 Val acc: 0.897 Val iou: 0.843\n",
      "New best model with val loss: 3.059e-01\n",
      "Epoch: 73 Train loss: 1.189e-02 Val loss: 3.044e-01 Val acc: 0.896 Val iou: 0.841\n",
      "New best model with val loss: 3.044e-01\n",
      "Epoch: 74 Train loss: 1.186e-02 Val loss: 3.043e-01 Val acc: 0.896 Val iou: 0.842\n",
      "New best model with val loss: 3.043e-01\n",
      "Epoch: 75 Train loss: 1.179e-02 Val loss: 3.042e-01 Val acc: 0.897 Val iou: 0.843\n",
      "New best model with val loss: 3.042e-01\n",
      "Epoch: 76 Train loss: 1.179e-02 Val loss: 3.032e-01 Val acc: 0.896 Val iou: 0.842\n",
      "New best model with val loss: 3.032e-01\n",
      "Epoch: 77 Train loss: 1.176e-02 Val loss: 3.035e-01 Val acc: 0.895 Val iou: 0.841\n",
      "Epoch: 78 Train loss: 1.178e-02 Val loss: 3.039e-01 Val acc: 0.895 Val iou: 0.840\n",
      "Epoch: 79 Train loss: 1.178e-02 Val loss: 3.036e-01 Val acc: 0.895 Val iou: 0.841\n",
      "Epoch: 80 Train loss: 1.176e-02 Val loss: 3.035e-01 Val acc: 0.894 Val iou: 0.840\n",
      "Epoch: 81 Train loss: 1.172e-02 Val loss: 3.011e-01 Val acc: 0.896 Val iou: 0.841\n",
      "New best model with val loss: 3.011e-01\n",
      "Epoch: 82 Train loss: 1.165e-02 Val loss: 3.026e-01 Val acc: 0.896 Val iou: 0.842\n",
      "Epoch: 83 Train loss: 1.170e-02 Val loss: 3.008e-01 Val acc: 0.897 Val iou: 0.842\n",
      "New best model with val loss: 3.008e-01\n",
      "Epoch: 84 Train loss: 1.165e-02 Val loss: 2.990e-01 Val acc: 0.895 Val iou: 0.840\n",
      "New best model with val loss: 2.990e-01\n",
      "Epoch: 85 Train loss: 1.163e-02 Val loss: 2.992e-01 Val acc: 0.896 Val iou: 0.842\n",
      "Epoch: 86 Train loss: 1.167e-02 Val loss: 3.017e-01 Val acc: 0.896 Val iou: 0.841\n",
      "Epoch: 87 Train loss: 1.162e-02 Val loss: 3.008e-01 Val acc: 0.896 Val iou: 0.841\n",
      "Epoch: 88 Train loss: 1.164e-02 Val loss: 3.006e-01 Val acc: 0.896 Val iou: 0.841\n",
      "Epoch: 89 Train loss: 1.159e-02 Val loss: 2.983e-01 Val acc: 0.895 Val iou: 0.840\n",
      "New best model with val loss: 2.983e-01\n",
      "Epoch: 90 Train loss: 1.159e-02 Val loss: 2.991e-01 Val acc: 0.896 Val iou: 0.842\n",
      "Epoch: 91 Train loss: 1.159e-02 Val loss: 2.995e-01 Val acc: 0.897 Val iou: 0.842\n",
      "Epoch: 92 Train loss: 1.162e-02 Val loss: 2.978e-01 Val acc: 0.896 Val iou: 0.842\n",
      "New best model with val loss: 2.978e-01\n",
      "Epoch: 93 Train loss: 1.158e-02 Val loss: 2.982e-01 Val acc: 0.898 Val iou: 0.844\n",
      "Epoch: 94 Train loss: 1.157e-02 Val loss: 2.976e-01 Val acc: 0.896 Val iou: 0.840\n",
      "New best model with val loss: 2.976e-01\n",
      "Epoch: 95 Train loss: 1.154e-02 Val loss: 2.973e-01 Val acc: 0.898 Val iou: 0.843\n",
      "New best model with val loss: 2.973e-01\n",
      "Epoch: 96 Train loss: 1.154e-02 Val loss: 2.972e-01 Val acc: 0.896 Val iou: 0.842\n",
      "New best model with val loss: 2.972e-01\n",
      "Epoch: 97 Train loss: 1.157e-02 Val loss: 2.964e-01 Val acc: 0.898 Val iou: 0.845\n",
      "New best model with val loss: 2.964e-01\n",
      "Epoch: 98 Train loss: 1.152e-02 Val loss: 2.974e-01 Val acc: 0.896 Val iou: 0.842\n",
      "Epoch: 99 Train loss: 1.158e-02 Val loss: 2.967e-01 Val acc: 0.896 Val iou: 0.842\n",
      "Training model with 4 hidden layers\n",
      "Epoch: 0 Train loss: 5.928e-02 Val loss: 1.458e+00 Val acc: 0.645 Val iou: 0.496\n",
      "New best model with val loss: 1.458e+00\n",
      "Epoch: 1 Train loss: 3.837e-02 Val loss: 8.385e-01 Val acc: 0.722 Val iou: 0.590\n",
      "New best model with val loss: 8.385e-01\n",
      "Epoch: 2 Train loss: 2.734e-02 Val loss: 6.577e-01 Val acc: 0.751 Val iou: 0.644\n",
      "New best model with val loss: 6.577e-01\n",
      "Epoch: 3 Train loss: 2.183e-02 Val loss: 5.146e-01 Val acc: 0.781 Val iou: 0.663\n",
      "New best model with val loss: 5.146e-01\n",
      "Epoch: 4 Train loss: 1.925e-02 Val loss: 4.768e-01 Val acc: 0.805 Val iou: 0.709\n",
      "New best model with val loss: 4.768e-01\n",
      "Epoch: 5 Train loss: 1.801e-02 Val loss: 4.560e-01 Val acc: 0.807 Val iou: 0.713\n",
      "New best model with val loss: 4.560e-01\n",
      "Epoch: 6 Train loss: 1.725e-02 Val loss: 4.415e-01 Val acc: 0.806 Val iou: 0.710\n",
      "New best model with val loss: 4.415e-01\n",
      "Epoch: 7 Train loss: 1.666e-02 Val loss: 4.316e-01 Val acc: 0.810 Val iou: 0.715\n",
      "New best model with val loss: 4.316e-01\n",
      "Epoch: 8 Train loss: 1.624e-02 Val loss: 4.232e-01 Val acc: 0.822 Val iou: 0.730\n",
      "New best model with val loss: 4.232e-01\n",
      "Epoch: 9 Train loss: 1.581e-02 Val loss: 4.144e-01 Val acc: 0.824 Val iou: 0.735\n",
      "New best model with val loss: 4.144e-01\n",
      "Epoch: 10 Train loss: 1.544e-02 Val loss: 4.090e-01 Val acc: 0.829 Val iou: 0.737\n",
      "New best model with val loss: 4.090e-01\n",
      "Epoch: 11 Train loss: 1.523e-02 Val loss: 4.062e-01 Val acc: 0.830 Val iou: 0.740\n",
      "New best model with val loss: 4.062e-01\n",
      "Epoch: 12 Train loss: 1.513e-02 Val loss: 4.010e-01 Val acc: 0.831 Val iou: 0.740\n",
      "New best model with val loss: 4.010e-01\n",
      "Epoch: 13 Train loss: 1.480e-02 Val loss: 3.975e-01 Val acc: 0.832 Val iou: 0.740\n",
      "New best model with val loss: 3.975e-01\n",
      "Epoch: 14 Train loss: 1.452e-02 Val loss: 3.923e-01 Val acc: 0.832 Val iou: 0.741\n",
      "New best model with val loss: 3.923e-01\n",
      "Epoch: 15 Train loss: 1.441e-02 Val loss: 3.891e-01 Val acc: 0.834 Val iou: 0.744\n",
      "New best model with val loss: 3.891e-01\n",
      "Epoch: 16 Train loss: 1.423e-02 Val loss: 3.875e-01 Val acc: 0.837 Val iou: 0.761\n",
      "New best model with val loss: 3.875e-01\n",
      "Epoch: 17 Train loss: 1.420e-02 Val loss: 3.840e-01 Val acc: 0.839 Val iou: 0.763\n",
      "New best model with val loss: 3.840e-01\n",
      "Epoch: 18 Train loss: 1.408e-02 Val loss: 3.824e-01 Val acc: 0.833 Val iou: 0.742\n",
      "New best model with val loss: 3.824e-01\n",
      "Epoch: 19 Train loss: 1.399e-02 Val loss: 3.793e-01 Val acc: 0.842 Val iou: 0.775\n",
      "New best model with val loss: 3.793e-01\n",
      "Epoch: 20 Train loss: 1.388e-02 Val loss: 3.774e-01 Val acc: 0.870 Val iou: 0.814\n",
      "New best model with val loss: 3.774e-01\n",
      "Epoch: 21 Train loss: 1.386e-02 Val loss: 3.747e-01 Val acc: 0.839 Val iou: 0.748\n",
      "New best model with val loss: 3.747e-01\n",
      "Epoch: 22 Train loss: 1.378e-02 Val loss: 3.740e-01 Val acc: 0.836 Val iou: 0.746\n",
      "New best model with val loss: 3.740e-01\n",
      "Epoch: 23 Train loss: 1.364e-02 Val loss: 3.724e-01 Val acc: 0.840 Val iou: 0.750\n",
      "New best model with val loss: 3.724e-01\n",
      "Epoch: 24 Train loss: 1.359e-02 Val loss: 3.684e-01 Val acc: 0.862 Val iou: 0.803\n",
      "New best model with val loss: 3.684e-01\n",
      "Epoch: 25 Train loss: 1.355e-02 Val loss: 3.679e-01 Val acc: 0.870 Val iou: 0.817\n",
      "New best model with val loss: 3.679e-01\n",
      "Epoch: 26 Train loss: 1.354e-02 Val loss: 3.653e-01 Val acc: 0.870 Val iou: 0.816\n",
      "New best model with val loss: 3.653e-01\n",
      "Epoch: 27 Train loss: 1.344e-02 Val loss: 3.641e-01 Val acc: 0.869 Val iou: 0.813\n",
      "New best model with val loss: 3.641e-01\n",
      "Epoch: 28 Train loss: 1.342e-02 Val loss: 3.677e-01 Val acc: 0.870 Val iou: 0.816\n",
      "Epoch: 29 Train loss: 1.340e-02 Val loss: 3.642e-01 Val acc: 0.875 Val iou: 0.818\n",
      "Epoch: 30 Train loss: 1.334e-02 Val loss: 3.608e-01 Val acc: 0.872 Val iou: 0.819\n",
      "New best model with val loss: 3.608e-01\n",
      "Epoch: 31 Train loss: 1.326e-02 Val loss: 3.598e-01 Val acc: 0.871 Val iou: 0.820\n",
      "New best model with val loss: 3.598e-01\n",
      "Epoch: 32 Train loss: 1.326e-02 Val loss: 3.605e-01 Val acc: 0.873 Val iou: 0.819\n",
      "Epoch: 33 Train loss: 1.326e-02 Val loss: 3.644e-01 Val acc: 0.872 Val iou: 0.816\n",
      "Epoch: 34 Train loss: 1.318e-02 Val loss: 3.594e-01 Val acc: 0.877 Val iou: 0.820\n",
      "New best model with val loss: 3.594e-01\n",
      "Epoch: 35 Train loss: 1.322e-02 Val loss: 3.633e-01 Val acc: 0.873 Val iou: 0.815\n",
      "Epoch: 36 Train loss: 1.313e-02 Val loss: 3.611e-01 Val acc: 0.875 Val iou: 0.818\n",
      "Epoch: 37 Train loss: 1.313e-02 Val loss: 3.576e-01 Val acc: 0.873 Val iou: 0.818\n",
      "New best model with val loss: 3.576e-01\n",
      "Epoch: 38 Train loss: 1.318e-02 Val loss: 3.625e-01 Val acc: 0.871 Val iou: 0.814\n",
      "Epoch: 39 Train loss: 1.310e-02 Val loss: 3.572e-01 Val acc: 0.874 Val iou: 0.818\n",
      "New best model with val loss: 3.572e-01\n",
      "Epoch: 40 Train loss: 1.302e-02 Val loss: 3.567e-01 Val acc: 0.875 Val iou: 0.818\n",
      "New best model with val loss: 3.567e-01\n",
      "Epoch: 41 Train loss: 1.295e-02 Val loss: 3.569e-01 Val acc: 0.874 Val iou: 0.816\n",
      "Epoch: 42 Train loss: 1.306e-02 Val loss: 3.570e-01 Val acc: 0.874 Val iou: 0.818\n",
      "Epoch: 43 Train loss: 1.295e-02 Val loss: 3.549e-01 Val acc: 0.877 Val iou: 0.818\n",
      "New best model with val loss: 3.549e-01\n",
      "Epoch: 44 Train loss: 1.290e-02 Val loss: 3.501e-01 Val acc: 0.875 Val iou: 0.817\n",
      "New best model with val loss: 3.501e-01\n",
      "Epoch: 45 Train loss: 1.292e-02 Val loss: 3.512e-01 Val acc: 0.877 Val iou: 0.818\n",
      "Epoch: 46 Train loss: 1.289e-02 Val loss: 3.534e-01 Val acc: 0.875 Val iou: 0.817\n",
      "Epoch: 47 Train loss: 1.288e-02 Val loss: 3.531e-01 Val acc: 0.875 Val iou: 0.817\n",
      "Epoch: 48 Train loss: 1.282e-02 Val loss: 3.579e-01 Val acc: 0.876 Val iou: 0.817\n",
      "Epoch: 49 Train loss: 1.280e-02 Val loss: 3.557e-01 Val acc: 0.876 Val iou: 0.817\n",
      "Epoch: 50 Train loss: 1.288e-02 Val loss: 3.525e-01 Val acc: 0.875 Val iou: 0.818\n",
      "Stopping early at epoch 50\n",
      "Training model with 8 hidden layers\n",
      "Epoch: 0 Train loss: 6.044e-02 Val loss: 1.504e+00 Val acc: 0.692 Val iou: 0.535\n",
      "New best model with val loss: 1.504e+00\n",
      "Epoch: 1 Train loss: 3.880e-02 Val loss: 8.684e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 8.684e-01\n",
      "Epoch: 2 Train loss: 2.760e-02 Val loss: 6.952e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.952e-01\n",
      "Epoch: 3 Train loss: 2.322e-02 Val loss: 6.351e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.351e-01\n",
      "Epoch: 4 Train loss: 2.149e-02 Val loss: 5.969e-01 Val acc: 0.706 Val iou: 0.558\n",
      "New best model with val loss: 5.969e-01\n",
      "Epoch: 5 Train loss: 2.053e-02 Val loss: 5.795e-01 Val acc: 0.729 Val iou: 0.598\n",
      "New best model with val loss: 5.795e-01\n",
      "Epoch: 6 Train loss: 1.990e-02 Val loss: 5.717e-01 Val acc: 0.726 Val iou: 0.604\n",
      "New best model with val loss: 5.717e-01\n",
      "Epoch: 7 Train loss: 1.953e-02 Val loss: 5.575e-01 Val acc: 0.725 Val iou: 0.610\n",
      "New best model with val loss: 5.575e-01\n",
      "Epoch: 8 Train loss: 1.923e-02 Val loss: 5.478e-01 Val acc: 0.751 Val iou: 0.655\n",
      "New best model with val loss: 5.478e-01\n",
      "Epoch: 9 Train loss: 1.887e-02 Val loss: 5.324e-01 Val acc: 0.763 Val iou: 0.674\n",
      "New best model with val loss: 5.324e-01\n",
      "Epoch: 10 Train loss: 1.837e-02 Val loss: 5.071e-01 Val acc: 0.787 Val iou: 0.687\n",
      "New best model with val loss: 5.071e-01\n",
      "Epoch: 11 Train loss: 1.753e-02 Val loss: 4.771e-01 Val acc: 0.798 Val iou: 0.689\n",
      "New best model with val loss: 4.771e-01\n",
      "Epoch: 12 Train loss: 1.709e-02 Val loss: 4.743e-01 Val acc: 0.795 Val iou: 0.684\n",
      "New best model with val loss: 4.743e-01\n",
      "Epoch: 13 Train loss: 1.656e-02 Val loss: 4.633e-01 Val acc: 0.807 Val iou: 0.700\n",
      "New best model with val loss: 4.633e-01\n",
      "Epoch: 14 Train loss: 1.638e-02 Val loss: 4.606e-01 Val acc: 0.805 Val iou: 0.700\n",
      "New best model with val loss: 4.606e-01\n",
      "Epoch: 15 Train loss: 1.608e-02 Val loss: 4.578e-01 Val acc: 0.804 Val iou: 0.698\n",
      "New best model with val loss: 4.578e-01\n",
      "Epoch: 16 Train loss: 1.596e-02 Val loss: 4.509e-01 Val acc: 0.810 Val iou: 0.706\n",
      "New best model with val loss: 4.509e-01\n",
      "Epoch: 17 Train loss: 1.575e-02 Val loss: 4.459e-01 Val acc: 0.810 Val iou: 0.708\n",
      "New best model with val loss: 4.459e-01\n",
      "Epoch: 18 Train loss: 1.563e-02 Val loss: 4.428e-01 Val acc: 0.813 Val iou: 0.711\n",
      "New best model with val loss: 4.428e-01\n",
      "Epoch: 19 Train loss: 1.553e-02 Val loss: 4.333e-01 Val acc: 0.819 Val iou: 0.719\n",
      "New best model with val loss: 4.333e-01\n",
      "Epoch: 20 Train loss: 1.550e-02 Val loss: 4.293e-01 Val acc: 0.818 Val iou: 0.721\n",
      "New best model with val loss: 4.293e-01\n",
      "Epoch: 21 Train loss: 1.538e-02 Val loss: 4.311e-01 Val acc: 0.813 Val iou: 0.714\n",
      "Epoch: 22 Train loss: 1.533e-02 Val loss: 4.307e-01 Val acc: 0.817 Val iou: 0.718\n",
      "Epoch: 23 Train loss: 1.525e-02 Val loss: 4.241e-01 Val acc: 0.821 Val iou: 0.723\n",
      "New best model with val loss: 4.241e-01\n",
      "Epoch: 24 Train loss: 1.518e-02 Val loss: 4.212e-01 Val acc: 0.824 Val iou: 0.724\n",
      "New best model with val loss: 4.212e-01\n",
      "Epoch: 25 Train loss: 1.510e-02 Val loss: 4.242e-01 Val acc: 0.821 Val iou: 0.722\n",
      "Epoch: 26 Train loss: 1.499e-02 Val loss: 4.247e-01 Val acc: 0.820 Val iou: 0.726\n",
      "Epoch: 27 Train loss: 1.494e-02 Val loss: 4.237e-01 Val acc: 0.822 Val iou: 0.725\n",
      "Epoch: 28 Train loss: 1.487e-02 Val loss: 4.197e-01 Val acc: 0.824 Val iou: 0.727\n",
      "New best model with val loss: 4.197e-01\n",
      "Epoch: 29 Train loss: 1.485e-02 Val loss: 4.144e-01 Val acc: 0.828 Val iou: 0.732\n",
      "New best model with val loss: 4.144e-01\n",
      "Epoch: 30 Train loss: 1.479e-02 Val loss: 4.175e-01 Val acc: 0.824 Val iou: 0.726\n",
      "Epoch: 31 Train loss: 1.477e-02 Val loss: 4.185e-01 Val acc: 0.821 Val iou: 0.724\n",
      "Epoch: 32 Train loss: 1.471e-02 Val loss: 4.188e-01 Val acc: 0.822 Val iou: 0.727\n",
      "Epoch: 33 Train loss: 1.471e-02 Val loss: 4.218e-01 Val acc: 0.822 Val iou: 0.727\n",
      "Epoch: 34 Train loss: 1.472e-02 Val loss: 4.159e-01 Val acc: 0.828 Val iou: 0.732\n",
      "Epoch: 35 Train loss: 1.462e-02 Val loss: 4.171e-01 Val acc: 0.823 Val iou: 0.727\n",
      "Stopping early at epoch 35\n",
      "Training model with 16 hidden layers\n",
      "Epoch: 0 Train loss: 6.065e-02 Val loss: 1.509e+00 Val acc: 0.582 Val iou: 0.419\n",
      "New best model with val loss: 1.509e+00\n",
      "Epoch: 1 Train loss: 3.851e-02 Val loss: 8.803e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 8.803e-01\n",
      "Epoch: 2 Train loss: 2.718e-02 Val loss: 6.913e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.913e-01\n",
      "Epoch: 3 Train loss: 2.340e-02 Val loss: 6.577e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.577e-01\n",
      "Epoch: 4 Train loss: 2.213e-02 Val loss: 6.437e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.437e-01\n",
      "Epoch: 5 Train loss: 2.143e-02 Val loss: 6.331e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.331e-01\n",
      "Epoch: 6 Train loss: 2.104e-02 Val loss: 6.246e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.246e-01\n",
      "Epoch: 7 Train loss: 2.070e-02 Val loss: 6.151e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.151e-01\n",
      "Epoch: 8 Train loss: 2.038e-02 Val loss: 6.076e-01 Val acc: 0.700 Val iou: 0.546\n",
      "New best model with val loss: 6.076e-01\n",
      "Epoch: 9 Train loss: 2.018e-02 Val loss: 6.009e-01 Val acc: 0.729 Val iou: 0.602\n",
      "New best model with val loss: 6.009e-01\n",
      "Epoch: 10 Train loss: 1.999e-02 Val loss: 5.950e-01 Val acc: 0.731 Val iou: 0.591\n",
      "New best model with val loss: 5.950e-01\n",
      "Epoch: 11 Train loss: 1.984e-02 Val loss: 5.931e-01 Val acc: 0.733 Val iou: 0.593\n",
      "New best model with val loss: 5.931e-01\n",
      "Epoch: 12 Train loss: 1.974e-02 Val loss: 5.983e-01 Val acc: 0.732 Val iou: 0.597\n",
      "Epoch: 13 Train loss: 1.974e-02 Val loss: 5.925e-01 Val acc: 0.735 Val iou: 0.596\n",
      "New best model with val loss: 5.925e-01\n",
      "Epoch: 14 Train loss: 1.951e-02 Val loss: 5.903e-01 Val acc: 0.733 Val iou: 0.595\n",
      "New best model with val loss: 5.903e-01\n",
      "Epoch: 15 Train loss: 1.948e-02 Val loss: 5.919e-01 Val acc: 0.733 Val iou: 0.594\n",
      "Epoch: 16 Train loss: 1.937e-02 Val loss: 5.881e-01 Val acc: 0.735 Val iou: 0.595\n",
      "New best model with val loss: 5.881e-01\n",
      "Epoch: 17 Train loss: 1.937e-02 Val loss: 5.955e-01 Val acc: 0.735 Val iou: 0.593\n",
      "Epoch: 18 Train loss: 1.927e-02 Val loss: 5.863e-01 Val acc: 0.738 Val iou: 0.597\n",
      "New best model with val loss: 5.863e-01\n",
      "Epoch: 19 Train loss: 1.925e-02 Val loss: 5.913e-01 Val acc: 0.735 Val iou: 0.591\n",
      "Epoch: 20 Train loss: 1.911e-02 Val loss: 5.826e-01 Val acc: 0.739 Val iou: 0.596\n",
      "New best model with val loss: 5.826e-01\n",
      "Epoch: 21 Train loss: 1.899e-02 Val loss: 5.845e-01 Val acc: 0.738 Val iou: 0.595\n",
      "Epoch: 22 Train loss: 1.886e-02 Val loss: 5.833e-01 Val acc: 0.738 Val iou: 0.595\n",
      "Epoch: 23 Train loss: 1.877e-02 Val loss: 5.883e-01 Val acc: 0.738 Val iou: 0.595\n",
      "Epoch: 24 Train loss: 1.867e-02 Val loss: 5.914e-01 Val acc: 0.739 Val iou: 0.596\n",
      "Epoch: 25 Train loss: 1.856e-02 Val loss: 6.109e-01 Val acc: 0.738 Val iou: 0.595\n",
      "Epoch: 26 Train loss: 1.841e-02 Val loss: 6.216e-01 Val acc: 0.738 Val iou: 0.595\n",
      "Stopping early at epoch 26\n"
     ]
    }
   ],
   "source": [
    "for num_hidden_layers in [2, 3, 4, 8, 16]:\n",
    "    model = GATModel(num_hidden_layers=num_hidden_layers)\n",
    "\n",
    "    print(f\"Training model with {num_hidden_layers} hidden layers\")\n",
    "\n",
    "    result = train_with_early_stopping(model, ds_train, ds_val, batch_size=32, learning_rate=0.001, n_epochs=100, device=\"cuda\", tolerance=5)\n",
    "\n",
    "    torch.save(result, f\"room_type_classifier_early_stopping_results_{num_hidden_layers}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3517662949824447\n",
      "0.29640644767352836\n",
      "0.3500914120930804\n",
      "0.41440538448865333\n",
      "0.5825848225771525\n"
     ]
    }
   ],
   "source": [
    "val_loss_dicts = []\n",
    "\n",
    "for num_hidden_layers in [2, 3, 4, 8, 16]:\n",
    "    result_i = torch.load( f\"room_type_classifier_early_stopping_results_{num_hidden_layers}.pt\", map_location=\"cpu\")\n",
    "\n",
    "    print(result_i[\"best_model_val_loss\"])\n",
    "\n",
    "    val_loss_dicts.append({\n",
    "        \"num_hidden_layers\": num_hidden_layers,\n",
    "        \"val_loss\": result_i[\"best_model_val_loss\"],\n",
    "        # \"max_epoch\": result_i[\"losses\"][-1][\"epoch\"],\n",
    "        \"val_acc\": result_i[\"best_model_loss_dict\"][\"val_acc\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['losses', 'best_model', 'best_model_val_loss', 'best_model_loss_dict', 'last_model'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_i.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='num_hidden_layers'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(val_loss_dicts).plot.bar(x=\"num_hidden_layers\", y=\"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrr}\n",
      "num hidden layers & val loss & val acc \\\\\n",
      "2 & 0.35 & 0.87 \\\\\n",
      "3 & 0.30 & 0.90 \\\\\n",
      "4 & 0.35 & 0.87 \\\\\n",
      "8 & 0.41 & 0.83 \\\\\n",
      "16 & 0.58 & 0.74 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(val_loss_dicts)[[\"num_hidden_layers\", \"val_loss\", \"val_acc\"]].style.hide(axis=\"index\").format(lambda x: f\"{x:.2f}\", [\"val_loss\", \"val_acc\"]).to_latex().replace(\"_\", \" \"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load one of the models from the hp search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which `num_hidden_layers` run to load:\n",
    "num_hidden_layers = 3\n",
    "results_dict = torch.load(f\"room_type_classifier_early_stopping_results_{num_hidden_layers}.pt\")\n",
    "\n",
    "model = results_dict[\"best_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATModel(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(4, 32, heads=1)\n",
       "    (1-2): 2 x GATConv(32, 32, heads=1)\n",
       "  )\n",
       "  (hidden_linear): Linear(in_features=36, out_features=32, bias=True)\n",
       "  (linear): Linear(in_features=32, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 80], zoning_type=[38, 4], connectivity=[80, 3], num_nodes=38, file_name='4167.pickle')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test = GraphZoningTypeTestSet('/path/to/modified-swiss-dwellings-v1-test/')\n",
    "\n",
    "ds_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GraphZoningTypeTestSet at 0x7f2340c57190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make folder for output graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_graph_path = ds_test.graph_path.replace(\"graph_in\", \"graph_pred\")\n",
    "\n",
    "os.makedirs(pred_graph_path, exist_ok=True)\n",
    "\n",
    "pred_graph_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make graph_pred graphs\n",
    "\n",
    "Run inference on the test graph_in files, and save the graph_pred graphs with the added room type attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4167.pickle\n",
      "4168.pickle\n",
      "4169.pickle\n",
      "4170.pickle\n",
      "4171.pickle\n",
      "4172.pickle\n",
      "4173.pickle\n",
      "4174.pickle\n",
      "4175.pickle\n",
      "4176.pickle\n",
      "4177.pickle\n",
      "4178.pickle\n",
      "4179.pickle\n",
      "4180.pickle\n",
      "4181.pickle\n",
      "4182.pickle\n",
      "4183.pickle\n",
      "4184.pickle\n",
      "4185.pickle\n",
      "4186.pickle\n",
      "4187.pickle\n",
      "4188.pickle\n",
      "4189.pickle\n",
      "4190.pickle\n",
      "4191.pickle\n",
      "4192.pickle\n",
      "4193.pickle\n",
      "4194.pickle\n",
      "4195.pickle\n",
      "4196.pickle\n",
      "4197.pickle\n",
      "4198.pickle\n",
      "4199.pickle\n",
      "4200.pickle\n",
      "4201.pickle\n",
      "4202.pickle\n",
      "4203.pickle\n",
      "4204.pickle\n",
      "4205.pickle\n",
      "4206.pickle\n",
      "4207.pickle\n",
      "4208.pickle\n",
      "4209.pickle\n",
      "4210.pickle\n",
      "4211.pickle\n",
      "4212.pickle\n",
      "4213.pickle\n",
      "4214.pickle\n",
      "4215.pickle\n",
      "4216.pickle\n",
      "4217.pickle\n",
      "4218.pickle\n",
      "4219.pickle\n",
      "4220.pickle\n",
      "4221.pickle\n",
      "4222.pickle\n",
      "4223.pickle\n",
      "4224.pickle\n",
      "4225.pickle\n",
      "4226.pickle\n",
      "4227.pickle\n",
      "4228.pickle\n",
      "4229.pickle\n",
      "4230.pickle\n",
      "4231.pickle\n",
      "4232.pickle\n",
      "4233.pickle\n",
      "4234.pickle\n",
      "4235.pickle\n",
      "4236.pickle\n",
      "4237.pickle\n",
      "4238.pickle\n",
      "4239.pickle\n",
      "4240.pickle\n",
      "4241.pickle\n",
      "4242.pickle\n",
      "4243.pickle\n",
      "4244.pickle\n",
      "4245.pickle\n",
      "4246.pickle\n",
      "4247.pickle\n",
      "4248.pickle\n",
      "4249.pickle\n",
      "4250.pickle\n",
      "4251.pickle\n",
      "4252.pickle\n",
      "4253.pickle\n",
      "4254.pickle\n",
      "4255.pickle\n",
      "4256.pickle\n",
      "4257.pickle\n",
      "4258.pickle\n",
      "4259.pickle\n",
      "4260.pickle\n",
      "4261.pickle\n",
      "4262.pickle\n",
      "4263.pickle\n",
      "4264.pickle\n",
      "4265.pickle\n",
      "4266.pickle\n",
      "4267.pickle\n",
      "4268.pickle\n",
      "4269.pickle\n",
      "4270.pickle\n",
      "4271.pickle\n",
      "4272.pickle\n",
      "4273.pickle\n",
      "4274.pickle\n",
      "4275.pickle\n",
      "4276.pickle\n",
      "4277.pickle\n",
      "4278.pickle\n",
      "4279.pickle\n",
      "4280.pickle\n",
      "4281.pickle\n",
      "4282.pickle\n",
      "4283.pickle\n",
      "4284.pickle\n",
      "4285.pickle\n",
      "4286.pickle\n",
      "4287.pickle\n",
      "4288.pickle\n",
      "4289.pickle\n",
      "4290.pickle\n",
      "4291.pickle\n",
      "4292.pickle\n",
      "4293.pickle\n",
      "4294.pickle\n",
      "4295.pickle\n",
      "4296.pickle\n",
      "4297.pickle\n",
      "4298.pickle\n",
      "4299.pickle\n",
      "4300.pickle\n",
      "4301.pickle\n",
      "4302.pickle\n",
      "4303.pickle\n",
      "4304.pickle\n",
      "4305.pickle\n",
      "4306.pickle\n",
      "4307.pickle\n",
      "4308.pickle\n",
      "4309.pickle\n",
      "4310.pickle\n",
      "4311.pickle\n",
      "4312.pickle\n",
      "4313.pickle\n",
      "4314.pickle\n",
      "4315.pickle\n",
      "4316.pickle\n",
      "4317.pickle\n",
      "4318.pickle\n",
      "4319.pickle\n",
      "4320.pickle\n",
      "4321.pickle\n",
      "4322.pickle\n",
      "4323.pickle\n",
      "4324.pickle\n",
      "4325.pickle\n",
      "4326.pickle\n",
      "4327.pickle\n",
      "4328.pickle\n",
      "4329.pickle\n",
      "4330.pickle\n",
      "4331.pickle\n",
      "4332.pickle\n",
      "4333.pickle\n",
      "4334.pickle\n",
      "4335.pickle\n",
      "4336.pickle\n",
      "4337.pickle\n",
      "4338.pickle\n",
      "4339.pickle\n",
      "4340.pickle\n",
      "4341.pickle\n",
      "4342.pickle\n",
      "4343.pickle\n",
      "4344.pickle\n",
      "4345.pickle\n",
      "4346.pickle\n",
      "4347.pickle\n",
      "4348.pickle\n",
      "4349.pickle\n",
      "4350.pickle\n",
      "4351.pickle\n",
      "4352.pickle\n",
      "4353.pickle\n",
      "4354.pickle\n",
      "4355.pickle\n",
      "4356.pickle\n",
      "4357.pickle\n",
      "4358.pickle\n",
      "4359.pickle\n",
      "4360.pickle\n",
      "4361.pickle\n",
      "4362.pickle\n",
      "4363.pickle\n",
      "4364.pickle\n",
      "4365.pickle\n",
      "4366.pickle\n",
      "4367.pickle\n",
      "4368.pickle\n",
      "4369.pickle\n",
      "4370.pickle\n",
      "4371.pickle\n",
      "4372.pickle\n",
      "4373.pickle\n",
      "4374.pickle\n",
      "4375.pickle\n",
      "4376.pickle\n",
      "4377.pickle\n",
      "4378.pickle\n",
      "4379.pickle\n",
      "4380.pickle\n",
      "4381.pickle\n",
      "4382.pickle\n",
      "4383.pickle\n",
      "4384.pickle\n",
      "4385.pickle\n",
      "4386.pickle\n",
      "4387.pickle\n",
      "4388.pickle\n",
      "4389.pickle\n",
      "4390.pickle\n",
      "4391.pickle\n",
      "4392.pickle\n",
      "4393.pickle\n",
      "4394.pickle\n",
      "4395.pickle\n",
      "4396.pickle\n",
      "4397.pickle\n",
      "4398.pickle\n",
      "4399.pickle\n",
      "4400.pickle\n",
      "4401.pickle\n",
      "4402.pickle\n",
      "4403.pickle\n",
      "4404.pickle\n",
      "4405.pickle\n",
      "4406.pickle\n",
      "4407.pickle\n",
      "4408.pickle\n",
      "4409.pickle\n",
      "4410.pickle\n",
      "4411.pickle\n",
      "4412.pickle\n",
      "4413.pickle\n",
      "4414.pickle\n",
      "4415.pickle\n",
      "4416.pickle\n",
      "4417.pickle\n",
      "4418.pickle\n",
      "4419.pickle\n",
      "4420.pickle\n",
      "4421.pickle\n",
      "4422.pickle\n",
      "4423.pickle\n",
      "4424.pickle\n",
      "4425.pickle\n",
      "4426.pickle\n",
      "4427.pickle\n",
      "4428.pickle\n",
      "4429.pickle\n",
      "4430.pickle\n",
      "4431.pickle\n",
      "4432.pickle\n",
      "4433.pickle\n",
      "4434.pickle\n",
      "4435.pickle\n",
      "4436.pickle\n",
      "4437.pickle\n",
      "4438.pickle\n",
      "4439.pickle\n",
      "4440.pickle\n",
      "4441.pickle\n",
      "4442.pickle\n",
      "4443.pickle\n",
      "4444.pickle\n",
      "4445.pickle\n",
      "4446.pickle\n",
      "4447.pickle\n",
      "4448.pickle\n",
      "4449.pickle\n",
      "4450.pickle\n",
      "4451.pickle\n",
      "4452.pickle\n",
      "4453.pickle\n",
      "4454.pickle\n",
      "4455.pickle\n",
      "4456.pickle\n",
      "4457.pickle\n",
      "4458.pickle\n",
      "4459.pickle\n",
      "4460.pickle\n",
      "4461.pickle\n",
      "4462.pickle\n",
      "4463.pickle\n",
      "4464.pickle\n",
      "4465.pickle\n",
      "4466.pickle\n",
      "4467.pickle\n",
      "4468.pickle\n",
      "4469.pickle\n",
      "4470.pickle\n",
      "4471.pickle\n",
      "4472.pickle\n",
      "4473.pickle\n",
      "4474.pickle\n",
      "4475.pickle\n",
      "4476.pickle\n",
      "4477.pickle\n",
      "4478.pickle\n",
      "4479.pickle\n",
      "4480.pickle\n",
      "4481.pickle\n",
      "4482.pickle\n",
      "4483.pickle\n",
      "4484.pickle\n",
      "4485.pickle\n",
      "4486.pickle\n",
      "4487.pickle\n",
      "4488.pickle\n",
      "4489.pickle\n",
      "4490.pickle\n",
      "4491.pickle\n",
      "4492.pickle\n",
      "4493.pickle\n",
      "4494.pickle\n",
      "4495.pickle\n",
      "4496.pickle\n",
      "4497.pickle\n",
      "4498.pickle\n",
      "4499.pickle\n",
      "4500.pickle\n",
      "4501.pickle\n",
      "4502.pickle\n",
      "4503.pickle\n",
      "4504.pickle\n",
      "4505.pickle\n",
      "4506.pickle\n",
      "4507.pickle\n",
      "4508.pickle\n",
      "4509.pickle\n",
      "4510.pickle\n",
      "4511.pickle\n",
      "4512.pickle\n",
      "4513.pickle\n",
      "4514.pickle\n",
      "4515.pickle\n",
      "4516.pickle\n",
      "4517.pickle\n",
      "4518.pickle\n",
      "4519.pickle\n",
      "4520.pickle\n",
      "4521.pickle\n",
      "4522.pickle\n",
      "4523.pickle\n",
      "4524.pickle\n",
      "4525.pickle\n",
      "4526.pickle\n",
      "4527.pickle\n",
      "4528.pickle\n",
      "4529.pickle\n",
      "4530.pickle\n",
      "4531.pickle\n",
      "4532.pickle\n",
      "4533.pickle\n",
      "4534.pickle\n",
      "4535.pickle\n",
      "4536.pickle\n",
      "4537.pickle\n",
      "4538.pickle\n",
      "4539.pickle\n",
      "4540.pickle\n",
      "4541.pickle\n",
      "4542.pickle\n",
      "4543.pickle\n",
      "4544.pickle\n",
      "4545.pickle\n",
      "4546.pickle\n",
      "4547.pickle\n",
      "4548.pickle\n",
      "4549.pickle\n",
      "4550.pickle\n",
      "4551.pickle\n",
      "4552.pickle\n",
      "4553.pickle\n",
      "4554.pickle\n",
      "4555.pickle\n",
      "4556.pickle\n",
      "4557.pickle\n",
      "4558.pickle\n",
      "4559.pickle\n",
      "4560.pickle\n",
      "4561.pickle\n",
      "4562.pickle\n",
      "4563.pickle\n",
      "4564.pickle\n",
      "4565.pickle\n",
      "4566.pickle\n",
      "4567.pickle\n",
      "4568.pickle\n",
      "4569.pickle\n",
      "4570.pickle\n",
      "4571.pickle\n",
      "4572.pickle\n",
      "4573.pickle\n",
      "4574.pickle\n",
      "4575.pickle\n",
      "4576.pickle\n",
      "4577.pickle\n",
      "4578.pickle\n",
      "4579.pickle\n",
      "4580.pickle\n",
      "4581.pickle\n",
      "4582.pickle\n",
      "4583.pickle\n",
      "4584.pickle\n",
      "4585.pickle\n",
      "4586.pickle\n",
      "4587.pickle\n",
      "4588.pickle\n",
      "4589.pickle\n",
      "4590.pickle\n",
      "4591.pickle\n",
      "4592.pickle\n",
      "4593.pickle\n",
      "4594.pickle\n",
      "4595.pickle\n",
      "4596.pickle\n",
      "4597.pickle\n",
      "4598.pickle\n",
      "4599.pickle\n",
      "4600.pickle\n",
      "4601.pickle\n",
      "4602.pickle\n",
      "4603.pickle\n",
      "4604.pickle\n",
      "4605.pickle\n",
      "4606.pickle\n",
      "4607.pickle\n",
      "4608.pickle\n",
      "4609.pickle\n",
      "4610.pickle\n",
      "4611.pickle\n",
      "4612.pickle\n",
      "4613.pickle\n",
      "4614.pickle\n",
      "4615.pickle\n",
      "4616.pickle\n",
      "4617.pickle\n",
      "4618.pickle\n",
      "4619.pickle\n",
      "4620.pickle\n",
      "4621.pickle\n",
      "4622.pickle\n",
      "4623.pickle\n",
      "4624.pickle\n",
      "4625.pickle\n",
      "4626.pickle\n",
      "4627.pickle\n",
      "4628.pickle\n",
      "4629.pickle\n",
      "4630.pickle\n",
      "4631.pickle\n",
      "4632.pickle\n",
      "4633.pickle\n",
      "4634.pickle\n",
      "4635.pickle\n",
      "4636.pickle\n",
      "4637.pickle\n",
      "4638.pickle\n",
      "4639.pickle\n",
      "4640.pickle\n",
      "4641.pickle\n",
      "4642.pickle\n",
      "4643.pickle\n",
      "4644.pickle\n",
      "4645.pickle\n",
      "4646.pickle\n",
      "4647.pickle\n",
      "4648.pickle\n",
      "4649.pickle\n",
      "4650.pickle\n",
      "4651.pickle\n",
      "4652.pickle\n",
      "4653.pickle\n",
      "4654.pickle\n",
      "4655.pickle\n",
      "4656.pickle\n",
      "4657.pickle\n",
      "4658.pickle\n",
      "4659.pickle\n",
      "4660.pickle\n",
      "4661.pickle\n",
      "4662.pickle\n",
      "4663.pickle\n",
      "4664.pickle\n",
      "4665.pickle\n",
      "4666.pickle\n",
      "4667.pickle\n",
      "4668.pickle\n",
      "4669.pickle\n",
      "4670.pickle\n",
      "4671.pickle\n",
      "4672.pickle\n",
      "4673.pickle\n",
      "4674.pickle\n",
      "4675.pickle\n",
      "4676.pickle\n",
      "4677.pickle\n",
      "4678.pickle\n",
      "4679.pickle\n",
      "4680.pickle\n",
      "4681.pickle\n",
      "4682.pickle\n",
      "4683.pickle\n",
      "4684.pickle\n",
      "4685.pickle\n",
      "4686.pickle\n",
      "4687.pickle\n",
      "4688.pickle\n",
      "4689.pickle\n",
      "4690.pickle\n",
      "4691.pickle\n",
      "4692.pickle\n",
      "4693.pickle\n",
      "4694.pickle\n",
      "4695.pickle\n",
      "4696.pickle\n",
      "4697.pickle\n",
      "4698.pickle\n",
      "4699.pickle\n",
      "4700.pickle\n",
      "4701.pickle\n",
      "4702.pickle\n",
      "4703.pickle\n",
      "4704.pickle\n",
      "4705.pickle\n",
      "4706.pickle\n",
      "4707.pickle\n",
      "4708.pickle\n",
      "4709.pickle\n",
      "4710.pickle\n",
      "4711.pickle\n",
      "4712.pickle\n",
      "4713.pickle\n",
      "4714.pickle\n",
      "4715.pickle\n",
      "4716.pickle\n",
      "4717.pickle\n",
      "4718.pickle\n",
      "4719.pickle\n",
      "4720.pickle\n",
      "4721.pickle\n",
      "4722.pickle\n",
      "4723.pickle\n",
      "4724.pickle\n",
      "4725.pickle\n",
      "4726.pickle\n",
      "4727.pickle\n",
      "4728.pickle\n",
      "4729.pickle\n",
      "4730.pickle\n",
      "4731.pickle\n",
      "4732.pickle\n",
      "4733.pickle\n",
      "4734.pickle\n",
      "4735.pickle\n",
      "4736.pickle\n",
      "4737.pickle\n",
      "4738.pickle\n",
      "4739.pickle\n",
      "4740.pickle\n",
      "4741.pickle\n",
      "4742.pickle\n",
      "4743.pickle\n",
      "4744.pickle\n",
      "4745.pickle\n",
      "4746.pickle\n",
      "4747.pickle\n",
      "4748.pickle\n",
      "4749.pickle\n",
      "4750.pickle\n",
      "4751.pickle\n",
      "4752.pickle\n",
      "4753.pickle\n",
      "4754.pickle\n",
      "4755.pickle\n",
      "4756.pickle\n",
      "4757.pickle\n",
      "4758.pickle\n",
      "4759.pickle\n",
      "4760.pickle\n",
      "4761.pickle\n",
      "4762.pickle\n",
      "4763.pickle\n",
      "4764.pickle\n",
      "4765.pickle\n",
      "4766.pickle\n",
      "4767.pickle\n",
      "4768.pickle\n",
      "4769.pickle\n",
      "4770.pickle\n",
      "4771.pickle\n",
      "4772.pickle\n",
      "4773.pickle\n",
      "4774.pickle\n",
      "4775.pickle\n",
      "4776.pickle\n",
      "4777.pickle\n",
      "4778.pickle\n",
      "4779.pickle\n",
      "4780.pickle\n",
      "4781.pickle\n",
      "4782.pickle\n",
      "4783.pickle\n",
      "4784.pickle\n",
      "4785.pickle\n",
      "4786.pickle\n",
      "4787.pickle\n",
      "4788.pickle\n",
      "4789.pickle\n",
      "4790.pickle\n",
      "4791.pickle\n",
      "4792.pickle\n",
      "4793.pickle\n",
      "4794.pickle\n",
      "4795.pickle\n",
      "4796.pickle\n",
      "4797.pickle\n",
      "4798.pickle\n",
      "4799.pickle\n",
      "4800.pickle\n",
      "4801.pickle\n",
      "4802.pickle\n",
      "4803.pickle\n",
      "4804.pickle\n",
      "4805.pickle\n",
      "4806.pickle\n",
      "4807.pickle\n",
      "4808.pickle\n",
      "4809.pickle\n",
      "4810.pickle\n",
      "4811.pickle\n",
      "4812.pickle\n",
      "4813.pickle\n",
      "4814.pickle\n",
      "4815.pickle\n",
      "4816.pickle\n",
      "4817.pickle\n",
      "4818.pickle\n",
      "4819.pickle\n",
      "4820.pickle\n",
      "4821.pickle\n",
      "4822.pickle\n",
      "4823.pickle\n",
      "4824.pickle\n",
      "4825.pickle\n",
      "4826.pickle\n",
      "4827.pickle\n",
      "4828.pickle\n",
      "4829.pickle\n",
      "4830.pickle\n",
      "4831.pickle\n",
      "4832.pickle\n",
      "4833.pickle\n",
      "4834.pickle\n",
      "4835.pickle\n",
      "4836.pickle\n",
      "4837.pickle\n",
      "4838.pickle\n",
      "4839.pickle\n",
      "4840.pickle\n",
      "4841.pickle\n",
      "4842.pickle\n",
      "4843.pickle\n",
      "4844.pickle\n",
      "4845.pickle\n",
      "4846.pickle\n",
      "4847.pickle\n",
      "4848.pickle\n",
      "4849.pickle\n",
      "4850.pickle\n",
      "4851.pickle\n",
      "4852.pickle\n",
      "4853.pickle\n",
      "4854.pickle\n",
      "4855.pickle\n",
      "4856.pickle\n",
      "4857.pickle\n",
      "4858.pickle\n",
      "4859.pickle\n",
      "4860.pickle\n",
      "4861.pickle\n",
      "4862.pickle\n",
      "4863.pickle\n",
      "4864.pickle\n",
      "4865.pickle\n",
      "4866.pickle\n",
      "4867.pickle\n",
      "4868.pickle\n",
      "4869.pickle\n",
      "4870.pickle\n",
      "4871.pickle\n",
      "4872.pickle\n",
      "4873.pickle\n",
      "4874.pickle\n",
      "4875.pickle\n",
      "4876.pickle\n",
      "4877.pickle\n",
      "4878.pickle\n",
      "4879.pickle\n",
      "4880.pickle\n",
      "4881.pickle\n",
      "4882.pickle\n",
      "4883.pickle\n",
      "4884.pickle\n",
      "4885.pickle\n",
      "4886.pickle\n",
      "4887.pickle\n",
      "4888.pickle\n",
      "4889.pickle\n",
      "4890.pickle\n",
      "4891.pickle\n",
      "4892.pickle\n",
      "4893.pickle\n",
      "4894.pickle\n",
      "4895.pickle\n",
      "4896.pickle\n",
      "4897.pickle\n",
      "4898.pickle\n",
      "4899.pickle\n",
      "4900.pickle\n",
      "4901.pickle\n",
      "4902.pickle\n",
      "4903.pickle\n",
      "4904.pickle\n",
      "4905.pickle\n",
      "4906.pickle\n",
      "4907.pickle\n",
      "4908.pickle\n",
      "4909.pickle\n",
      "4910.pickle\n",
      "4911.pickle\n",
      "4912.pickle\n",
      "4913.pickle\n",
      "4914.pickle\n",
      "4915.pickle\n",
      "4916.pickle\n",
      "4917.pickle\n",
      "4918.pickle\n",
      "4919.pickle\n",
      "4920.pickle\n",
      "4921.pickle\n",
      "4922.pickle\n",
      "4923.pickle\n",
      "4924.pickle\n",
      "4925.pickle\n",
      "4926.pickle\n",
      "4927.pickle\n",
      "4928.pickle\n",
      "4929.pickle\n",
      "4930.pickle\n",
      "4931.pickle\n",
      "4932.pickle\n",
      "4933.pickle\n",
      "4934.pickle\n",
      "4935.pickle\n",
      "4936.pickle\n",
      "4937.pickle\n",
      "4938.pickle\n",
      "4939.pickle\n",
      "4940.pickle\n",
      "4941.pickle\n",
      "4942.pickle\n",
      "4943.pickle\n",
      "4944.pickle\n",
      "4945.pickle\n",
      "4946.pickle\n",
      "4947.pickle\n",
      "4948.pickle\n",
      "4949.pickle\n",
      "4950.pickle\n",
      "4951.pickle\n",
      "4952.pickle\n",
      "4953.pickle\n",
      "4954.pickle\n",
      "4955.pickle\n",
      "4956.pickle\n",
      "4957.pickle\n",
      "4958.pickle\n",
      "4959.pickle\n",
      "4960.pickle\n",
      "4961.pickle\n",
      "4962.pickle\n",
      "4963.pickle\n",
      "4964.pickle\n",
      "4965.pickle\n",
      "4966.pickle\n",
      "4967.pickle\n",
      "4968.pickle\n",
      "4969.pickle\n",
      "4970.pickle\n",
      "4971.pickle\n",
      "4972.pickle\n",
      "4973.pickle\n",
      "4974.pickle\n",
      "4975.pickle\n",
      "4976.pickle\n",
      "4977.pickle\n",
      "4978.pickle\n",
      "4979.pickle\n",
      "4980.pickle\n",
      "4981.pickle\n",
      "4982.pickle\n",
      "4983.pickle\n",
      "4984.pickle\n",
      "4985.pickle\n",
      "4986.pickle\n",
      "4987.pickle\n",
      "4988.pickle\n",
      "4989.pickle\n",
      "4990.pickle\n",
      "4991.pickle\n",
      "4992.pickle\n",
      "4993.pickle\n",
      "4994.pickle\n",
      "4995.pickle\n",
      "4996.pickle\n",
      "4997.pickle\n",
      "4998.pickle\n",
      "4999.pickle\n",
      "5000.pickle\n",
      "5001.pickle\n",
      "5002.pickle\n",
      "5003.pickle\n",
      "5004.pickle\n",
      "5005.pickle\n",
      "5006.pickle\n",
      "5007.pickle\n",
      "5008.pickle\n",
      "5009.pickle\n",
      "5010.pickle\n",
      "5011.pickle\n",
      "5012.pickle\n",
      "5013.pickle\n",
      "5014.pickle\n",
      "5015.pickle\n",
      "5016.pickle\n",
      "5017.pickle\n",
      "5018.pickle\n",
      "5019.pickle\n",
      "5020.pickle\n",
      "5021.pickle\n",
      "5022.pickle\n",
      "5023.pickle\n",
      "5024.pickle\n",
      "5025.pickle\n",
      "5026.pickle\n",
      "5027.pickle\n",
      "5028.pickle\n",
      "5029.pickle\n",
      "5030.pickle\n",
      "5031.pickle\n",
      "5032.pickle\n",
      "5033.pickle\n",
      "5034.pickle\n",
      "5035.pickle\n",
      "5036.pickle\n",
      "5037.pickle\n",
      "5038.pickle\n",
      "5039.pickle\n",
      "5040.pickle\n",
      "5041.pickle\n",
      "5042.pickle\n",
      "5043.pickle\n",
      "5044.pickle\n",
      "5045.pickle\n",
      "5046.pickle\n",
      "5047.pickle\n",
      "5048.pickle\n",
      "5049.pickle\n",
      "5050.pickle\n",
      "5051.pickle\n",
      "5052.pickle\n",
      "5053.pickle\n",
      "5054.pickle\n",
      "5055.pickle\n",
      "5056.pickle\n",
      "5057.pickle\n",
      "5058.pickle\n",
      "5059.pickle\n",
      "5060.pickle\n",
      "5061.pickle\n",
      "5062.pickle\n",
      "5063.pickle\n",
      "5064.pickle\n",
      "5065.pickle\n",
      "5066.pickle\n",
      "5067.pickle\n",
      "5068.pickle\n",
      "5069.pickle\n",
      "5070.pickle\n",
      "5071.pickle\n",
      "5072.pickle\n",
      "5073.pickle\n",
      "5074.pickle\n",
      "5075.pickle\n",
      "5076.pickle\n",
      "5077.pickle\n",
      "5078.pickle\n",
      "5079.pickle\n",
      "5080.pickle\n",
      "5081.pickle\n",
      "5082.pickle\n",
      "5083.pickle\n",
      "5084.pickle\n",
      "5085.pickle\n",
      "5086.pickle\n",
      "5087.pickle\n",
      "5088.pickle\n",
      "5089.pickle\n",
      "5090.pickle\n",
      "5091.pickle\n",
      "5092.pickle\n",
      "5093.pickle\n",
      "5094.pickle\n",
      "5095.pickle\n",
      "5096.pickle\n",
      "5097.pickle\n",
      "5098.pickle\n",
      "5099.pickle\n",
      "5100.pickle\n",
      "5101.pickle\n",
      "5102.pickle\n",
      "5103.pickle\n",
      "5104.pickle\n",
      "5105.pickle\n",
      "5106.pickle\n",
      "5107.pickle\n",
      "5108.pickle\n",
      "5109.pickle\n",
      "5110.pickle\n",
      "5111.pickle\n",
      "5112.pickle\n",
      "5113.pickle\n",
      "5114.pickle\n",
      "5115.pickle\n",
      "5116.pickle\n",
      "5117.pickle\n",
      "5118.pickle\n",
      "5119.pickle\n",
      "5120.pickle\n",
      "5121.pickle\n",
      "5122.pickle\n",
      "5123.pickle\n",
      "5124.pickle\n",
      "5125.pickle\n",
      "5126.pickle\n",
      "5127.pickle\n",
      "5128.pickle\n",
      "5129.pickle\n",
      "5130.pickle\n",
      "5131.pickle\n",
      "5132.pickle\n",
      "5133.pickle\n",
      "5134.pickle\n",
      "5135.pickle\n",
      "5136.pickle\n",
      "5137.pickle\n",
      "5138.pickle\n",
      "5139.pickle\n",
      "5140.pickle\n",
      "5141.pickle\n",
      "5142.pickle\n",
      "5143.pickle\n",
      "5144.pickle\n",
      "5145.pickle\n",
      "5146.pickle\n",
      "5147.pickle\n",
      "5148.pickle\n",
      "5149.pickle\n",
      "5150.pickle\n",
      "5151.pickle\n",
      "5152.pickle\n",
      "5153.pickle\n",
      "5154.pickle\n",
      "5155.pickle\n",
      "5156.pickle\n",
      "5157.pickle\n",
      "5158.pickle\n",
      "5159.pickle\n",
      "5160.pickle\n",
      "5161.pickle\n",
      "5162.pickle\n",
      "5163.pickle\n",
      "5164.pickle\n",
      "5165.pickle\n",
      "5166.pickle\n",
      "5167.pickle\n",
      "5168.pickle\n",
      "5169.pickle\n",
      "5170.pickle\n",
      "5171.pickle\n",
      "5172.pickle\n",
      "5173.pickle\n",
      "5174.pickle\n",
      "5175.pickle\n",
      "5176.pickle\n",
      "5177.pickle\n",
      "5178.pickle\n",
      "5179.pickle\n",
      "5180.pickle\n",
      "5181.pickle\n",
      "5182.pickle\n",
      "5183.pickle\n",
      "5184.pickle\n",
      "5185.pickle\n",
      "5186.pickle\n",
      "5187.pickle\n",
      "5188.pickle\n",
      "5189.pickle\n",
      "5190.pickle\n",
      "5191.pickle\n",
      "5192.pickle\n",
      "5193.pickle\n",
      "5194.pickle\n",
      "5195.pickle\n",
      "5196.pickle\n",
      "5197.pickle\n",
      "5198.pickle\n",
      "5199.pickle\n",
      "5200.pickle\n",
      "5201.pickle\n",
      "5202.pickle\n",
      "5203.pickle\n",
      "5204.pickle\n",
      "5205.pickle\n",
      "5206.pickle\n",
      "5207.pickle\n",
      "5208.pickle\n",
      "5209.pickle\n",
      "5210.pickle\n",
      "5211.pickle\n",
      "5212.pickle\n",
      "5213.pickle\n",
      "5214.pickle\n",
      "5215.pickle\n",
      "5216.pickle\n",
      "5217.pickle\n",
      "5218.pickle\n",
      "5219.pickle\n",
      "5220.pickle\n",
      "5221.pickle\n",
      "5222.pickle\n",
      "5223.pickle\n",
      "5224.pickle\n",
      "5225.pickle\n",
      "5226.pickle\n",
      "5227.pickle\n",
      "5228.pickle\n",
      "5229.pickle\n",
      "5230.pickle\n",
      "5231.pickle\n",
      "5232.pickle\n",
      "5233.pickle\n",
      "5234.pickle\n",
      "5235.pickle\n",
      "5236.pickle\n",
      "5237.pickle\n",
      "5238.pickle\n",
      "5239.pickle\n",
      "5240.pickle\n",
      "5241.pickle\n",
      "5242.pickle\n",
      "5243.pickle\n",
      "5244.pickle\n",
      "5245.pickle\n",
      "5246.pickle\n",
      "5247.pickle\n",
      "5248.pickle\n",
      "5249.pickle\n",
      "5250.pickle\n",
      "5251.pickle\n",
      "5252.pickle\n",
      "5253.pickle\n",
      "5254.pickle\n",
      "5255.pickle\n",
      "5256.pickle\n",
      "5257.pickle\n",
      "5258.pickle\n",
      "5259.pickle\n",
      "5260.pickle\n",
      "5261.pickle\n",
      "5262.pickle\n",
      "5263.pickle\n",
      "5264.pickle\n",
      "5265.pickle\n",
      "5266.pickle\n",
      "5267.pickle\n",
      "5268.pickle\n",
      "5269.pickle\n",
      "5270.pickle\n",
      "5271.pickle\n",
      "5272.pickle\n",
      "5273.pickle\n",
      "5274.pickle\n",
      "5275.pickle\n",
      "5276.pickle\n",
      "5277.pickle\n",
      "5278.pickle\n",
      "5279.pickle\n",
      "5280.pickle\n",
      "5281.pickle\n",
      "5282.pickle\n",
      "5283.pickle\n",
      "5284.pickle\n",
      "5285.pickle\n",
      "5286.pickle\n",
      "5287.pickle\n",
      "5288.pickle\n",
      "5289.pickle\n",
      "5290.pickle\n",
      "5291.pickle\n",
      "5292.pickle\n",
      "5293.pickle\n",
      "5294.pickle\n",
      "5295.pickle\n",
      "5296.pickle\n",
      "5297.pickle\n",
      "5298.pickle\n",
      "5299.pickle\n",
      "5300.pickle\n",
      "5301.pickle\n",
      "5302.pickle\n",
      "5303.pickle\n",
      "5304.pickle\n",
      "5305.pickle\n",
      "5306.pickle\n",
      "5307.pickle\n",
      "5308.pickle\n",
      "5309.pickle\n",
      "5310.pickle\n",
      "5311.pickle\n",
      "5312.pickle\n",
      "5313.pickle\n",
      "5314.pickle\n",
      "5315.pickle\n",
      "5316.pickle\n",
      "5317.pickle\n",
      "5318.pickle\n",
      "5319.pickle\n",
      "5320.pickle\n",
      "5321.pickle\n",
      "5322.pickle\n",
      "5323.pickle\n",
      "5324.pickle\n",
      "5325.pickle\n",
      "5326.pickle\n",
      "5327.pickle\n",
      "5328.pickle\n",
      "5329.pickle\n",
      "5330.pickle\n",
      "5331.pickle\n",
      "5332.pickle\n",
      "5333.pickle\n",
      "5334.pickle\n",
      "5335.pickle\n",
      "5336.pickle\n",
      "5337.pickle\n",
      "5338.pickle\n",
      "5339.pickle\n",
      "5340.pickle\n",
      "5341.pickle\n",
      "5342.pickle\n",
      "5343.pickle\n",
      "5344.pickle\n",
      "5345.pickle\n",
      "5346.pickle\n",
      "5347.pickle\n",
      "5348.pickle\n",
      "5349.pickle\n",
      "5350.pickle\n",
      "5351.pickle\n",
      "5352.pickle\n",
      "5353.pickle\n",
      "5354.pickle\n",
      "5355.pickle\n",
      "5356.pickle\n",
      "5357.pickle\n",
      "5358.pickle\n",
      "5359.pickle\n",
      "5360.pickle\n",
      "5361.pickle\n",
      "5362.pickle\n",
      "5363.pickle\n",
      "5364.pickle\n",
      "5365.pickle\n",
      "5366.pickle\n",
      "5367.pickle\n",
      "5368.pickle\n",
      "5369.pickle\n",
      "5370.pickle\n",
      "5371.pickle\n",
      "5372.pickle\n",
      "5373.pickle\n",
      "5374.pickle\n",
      "5375.pickle\n",
      "5376.pickle\n",
      "5377.pickle\n",
      "5378.pickle\n",
      "5379.pickle\n",
      "5380.pickle\n",
      "5381.pickle\n",
      "5382.pickle\n",
      "5383.pickle\n",
      "5384.pickle\n",
      "5385.pickle\n",
      "5386.pickle\n",
      "5387.pickle\n",
      "5388.pickle\n",
      "5389.pickle\n",
      "5390.pickle\n",
      "5391.pickle\n",
      "5392.pickle\n",
      "5393.pickle\n",
      "5394.pickle\n",
      "5395.pickle\n",
      "5396.pickle\n",
      "5397.pickle\n",
      "5398.pickle\n",
      "5399.pickle\n",
      "5400.pickle\n",
      "5401.pickle\n",
      "5402.pickle\n",
      "5403.pickle\n",
      "5404.pickle\n",
      "5405.pickle\n",
      "5406.pickle\n",
      "5407.pickle\n",
      "5408.pickle\n",
      "5409.pickle\n",
      "5410.pickle\n",
      "5411.pickle\n",
      "5412.pickle\n",
      "5413.pickle\n",
      "5414.pickle\n",
      "5415.pickle\n",
      "5416.pickle\n",
      "5417.pickle\n",
      "5418.pickle\n",
      "5419.pickle\n",
      "5420.pickle\n",
      "5421.pickle\n",
      "5422.pickle\n",
      "5423.pickle\n",
      "5424.pickle\n",
      "5425.pickle\n",
      "5426.pickle\n",
      "5427.pickle\n",
      "5428.pickle\n",
      "5429.pickle\n",
      "5430.pickle\n",
      "5431.pickle\n",
      "5432.pickle\n",
      "5433.pickle\n",
      "5434.pickle\n",
      "5435.pickle\n",
      "5436.pickle\n",
      "5437.pickle\n",
      "5438.pickle\n",
      "5439.pickle\n",
      "5440.pickle\n",
      "5441.pickle\n",
      "5442.pickle\n",
      "5443.pickle\n",
      "5444.pickle\n",
      "5445.pickle\n",
      "5446.pickle\n",
      "5447.pickle\n",
      "5448.pickle\n",
      "5449.pickle\n",
      "5450.pickle\n",
      "5451.pickle\n",
      "5452.pickle\n",
      "5453.pickle\n",
      "5454.pickle\n",
      "5455.pickle\n",
      "5456.pickle\n",
      "5457.pickle\n",
      "5458.pickle\n",
      "5459.pickle\n",
      "5460.pickle\n",
      "5461.pickle\n",
      "5462.pickle\n",
      "5463.pickle\n",
      "5464.pickle\n",
      "5465.pickle\n",
      "5466.pickle\n",
      "5467.pickle\n",
      "5468.pickle\n",
      "5469.pickle\n",
      "5470.pickle\n",
      "5471.pickle\n",
      "5472.pickle\n",
      "5473.pickle\n",
      "5474.pickle\n",
      "5475.pickle\n",
      "5476.pickle\n",
      "5477.pickle\n",
      "5478.pickle\n",
      "5479.pickle\n",
      "5480.pickle\n",
      "5481.pickle\n",
      "5482.pickle\n",
      "5483.pickle\n",
      "5484.pickle\n",
      "5485.pickle\n",
      "5486.pickle\n",
      "5487.pickle\n",
      "5488.pickle\n",
      "5489.pickle\n",
      "5490.pickle\n",
      "5491.pickle\n",
      "5492.pickle\n",
      "5493.pickle\n",
      "5494.pickle\n",
      "5495.pickle\n",
      "5496.pickle\n",
      "5497.pickle\n",
      "5498.pickle\n",
      "5499.pickle\n",
      "5500.pickle\n",
      "5501.pickle\n",
      "5502.pickle\n",
      "5503.pickle\n",
      "5504.pickle\n",
      "5505.pickle\n",
      "5506.pickle\n",
      "5507.pickle\n",
      "5508.pickle\n",
      "5509.pickle\n",
      "5510.pickle\n",
      "5511.pickle\n",
      "5512.pickle\n",
      "5513.pickle\n",
      "5514.pickle\n",
      "5515.pickle\n",
      "5516.pickle\n",
      "5517.pickle\n",
      "5518.pickle\n",
      "5519.pickle\n",
      "5520.pickle\n",
      "5521.pickle\n",
      "5522.pickle\n",
      "5523.pickle\n",
      "5524.pickle\n",
      "5525.pickle\n",
      "5526.pickle\n",
      "5527.pickle\n",
      "5528.pickle\n",
      "5529.pickle\n",
      "5530.pickle\n",
      "5531.pickle\n",
      "5532.pickle\n",
      "5533.pickle\n",
      "5534.pickle\n",
      "5535.pickle\n",
      "5536.pickle\n",
      "5537.pickle\n",
      "5538.pickle\n",
      "5539.pickle\n",
      "5540.pickle\n",
      "5541.pickle\n",
      "5542.pickle\n",
      "5543.pickle\n",
      "5544.pickle\n",
      "5545.pickle\n",
      "5546.pickle\n",
      "5547.pickle\n",
      "5548.pickle\n",
      "5549.pickle\n",
      "5550.pickle\n",
      "5551.pickle\n",
      "5552.pickle\n",
      "5553.pickle\n",
      "5554.pickle\n",
      "5555.pickle\n",
      "5556.pickle\n"
     ]
    }
   ],
   "source": [
    "def inference(model, data_test, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in DataLoader(data_test, batch_size=1):\n",
    "            data = data.to(device)\n",
    "            pred = torch.argmax(model(data), dim=-1)\n",
    "\n",
    "            pred = pred.cpu().numpy()\n",
    "\n",
    "            file_name = data.file_name\n",
    "\n",
    "            assert len(file_name) == 1\n",
    "\n",
    "            yield file_name[0], pred\n",
    "\n",
    "inv_room_mapping = {val: key for key, val in constants.ROOM_MAPPING.items()}\n",
    "\n",
    "for file_name, pred in inference(model, ds_test, device=\"cpu\"):\n",
    "    graph_nx = load_pickle(os.path.join(ds_test.graph_path, file_name))\n",
    "\n",
    "    for node in graph_nx.nodes:\n",
    "        graph_nx.nodes[node]['room_type'] = pred[node]\n",
    "\n",
    "    # print(graph_nx.nodes(data=True))\n",
    "\n",
    "    print(file_name)\n",
    "\n",
    "    for node in graph_nx.nodes:\n",
    "        room_name = constants.ROOM_NAMES[graph_nx.nodes[node]['room_type']]\n",
    "\n",
    "        room_name = inv_room_mapping[room_name]\n",
    "\n",
    "        pred_zone_name = constants.ZONING_MAPPING[room_name]\n",
    "\n",
    "        zone_name = constants.ZONING_NAMES[graph_nx.nodes[node]['zoning_type']]\n",
    "\n",
    "        # Check that the predictions make sense according to the room type -> zoning type mapping\n",
    "        assert zone_name == pred_zone_name, f\"Zone name: {zone_name} Pred zone name: {pred_zone_name}\"\n",
    "    \n",
    "    save_pickle(graph_nx, os.path.join(pred_graph_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATModel(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(4, 32, heads=1)\n",
       "    (1): GATConv(32, 32, heads=1)\n",
       "  )\n",
       "  (linear): Linear(in_features=32, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
