{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting number of corners per room\n",
    "\n",
    "In the HouseDiffusion paper, the number of corners per room are sampled conditioned only on the room_type. They did it by counting how often each number_of_corners was used per room type, and sampling each room independently based on the room type.\n",
    "\n",
    "As I already had a notebook for predicting the room type from the zoning type, I tried using the same method for predicting number of corners. However the accuracy turned out to be quite a bit lower for predicting number of corners compared to predicting room type.\n",
    "\n",
    "\n",
    "This notebook is based on the `node_classification_room_type.ipynb` one. The `node_classification_room_type.ipynb` has to be run first, as this notebook uses the outputted room_type as feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from utils import load_pickle\n",
    "\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing.process_graphs import simplify_room_polygon\n",
    "from shapely import geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zoning_attribute(graph: nx.Graph):\n",
    "    room_type = nx.get_node_attributes(graph, 'room_type')\n",
    "\n",
    "    room_names = {node: constants.ROOM_NAMES[value] for node, value in room_type.items()}\n",
    "\n",
    "    inv_room_mapping = {val: key for key, val in constants.ROOM_MAPPING.items()}\n",
    "\n",
    "    room_names = {node: inv_room_mapping[value] for node, value in room_names.items()}\n",
    "\n",
    "    zoning = {key: constants.ZONING_MAPPING[value] for key, value in room_names.items()}\n",
    "\n",
    "    zoning_index = {key: constants.ZONING_NAMES.index(value) for key, value in zoning.items()}\n",
    "\n",
    "    nx.set_node_attributes(graph, zoning_index, 'zoning_type')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_CORNERS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def one_hot_encode(value, num_classes):\n",
    "    return torch.eye(num_classes)[value]\n",
    "\n",
    "\n",
    "NUM_ROOM_TYPES = 9\n",
    "NUM_ZONING_TYPES = 4\n",
    "\n",
    "# def one_hot_encode_types(graph: nx.Graph):\n",
    "#     for node in graph.nodes:\n",
    "#         graph.nodes[node]['room_type'] = one_hot_encode(graph.nodes[node]['room_type'], NUM_ROOM_TYPES)\n",
    "#         graph.nodes[node]['zoning_type'] = one_hot_encode(graph.nodes[node]['zoning_type'], NUM_ZONING_TYPES)\n",
    "\n",
    "CONNECTIVITIES = [\"door\", \"entrance\", \"passage\"]\n",
    "\n",
    "class GraphZoningRoomTypeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Graph Dataset. Collects NetworkX graph from a pre-defined folder and\n",
    "    transforms them to Pytorch Geometric (pyg.data.Data()) instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, split=\"train\"):\n",
    "        # self.graph_in_path = os.path.join(path, 'graph_in')\n",
    "        self.graph_out_path = os.path.join(path, 'graph_out')\n",
    "\n",
    "        # include graph transformations if you like\n",
    "        # self.graph_transform = graph_transform\n",
    "\n",
    "        all_files = os.listdir(self.graph_out_path)\n",
    "\n",
    "        self.train_files, self.val_files = train_test_split(all_files, test_size=0.05, random_state=42)\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.files = self.train_files\n",
    "        elif split == \"val\":\n",
    "            self.files = self.val_files\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "        \n",
    "        self.cache = {}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if index in self.cache:\n",
    "            graph_nx = self.cache[index]\n",
    "        else:\n",
    "            file_name = self.files[index]\n",
    "\n",
    "            # get access graph (name is index)\n",
    "            graph_nx = load_pickle(os.path.join(self.graph_out_path, file_name))\n",
    "\n",
    "            add_zoning_attribute(graph_nx)\n",
    "\n",
    "            for edge in graph_nx.edges:\n",
    "                graph_nx.edges[edge]['connectivity'] = CONNECTIVITIES.index(graph_nx.edges[edge]['connectivity'])\n",
    "\n",
    "            for node in graph_nx.nodes:\n",
    "                polygon = geometry.Polygon(graph_nx.nodes[node]['geometry'])\n",
    "\n",
    "                polygon = simplify_room_polygon(polygon)\n",
    "\n",
    "                graph_nx.nodes[node]['n_corners'] = float(len(polygon.exterior.coords))\n",
    "\n",
    "                del graph_nx.nodes[node]['geometry']\n",
    "\n",
    "                # graph_nx.nodes[node][\"polygon\"] = np.array(polygon.exterior.coords)\n",
    "\n",
    "            # Remove attributes geometry, centroid\n",
    "            for node in graph_nx.nodes:\n",
    "                # graph_nx.nodes[node].pop('geometry', None)\n",
    "                graph_nx.nodes[node].pop('centroid', None)\n",
    "\n",
    "            # graph_nx.graph[]\n",
    "\n",
    "            self.cache[index] = graph_nx\n",
    "\n",
    "        # transform networkx graph to pytorch geometric graph\n",
    "        graph_pyg = pyg.utils.from_networkx(graph_nx)\n",
    "\n",
    "        # transform graph if you like\n",
    "        graph_pyg = self.graph_transform(graph_pyg)\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    @staticmethod\n",
    "    def graph_transform(graph_pyg):\n",
    "        graph_pyg[\"room_type\"] = one_hot_encode(graph_pyg[\"room_type\"], NUM_ROOM_TYPES)\n",
    "        graph_pyg[\"zoning_type\"] = one_hot_encode(graph_pyg[\"zoning_type\"], NUM_ZONING_TYPES)\n",
    "\n",
    "        graph_pyg[\"connectivity\"] = one_hot_encode(graph_pyg[\"connectivity\"], len(CONNECTIVITIES))\n",
    "\n",
    "        # graph_pyg[\"n_corners\"] = one_hot_encode(graph_pyg[\"n_corners\"], MAX_CORNERS)\n",
    "\n",
    "        graph_pyg[\"n_corners\"] = torch.unsqueeze(graph_pyg[\"n_corners\"], dim=-1)\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphZoningTypeTestSet(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Graph Dataset. Collects NetworkX graph from a pre-defined folder and\n",
    "    transforms them to Pytorch Geometric (pyg.data.Data()) instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, graph_name=\"graph_pred\"):\n",
    "        self.graph_path = os.path.join(path, graph_name)\n",
    "\n",
    "        self.files = os.listdir(self.graph_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file_name = self.files[index]\n",
    "\n",
    "        # get access graph (name is index)\n",
    "        graph_nx = load_pickle(os.path.join(self.graph_path, file_name))\n",
    "\n",
    "        for edge in graph_nx.edges:\n",
    "            graph_nx.edges[edge]['connectivity'] = CONNECTIVITIES.index(graph_nx.edges[edge]['connectivity'])\n",
    "\n",
    "        # transform networkx graph to pytorch geometric graph\n",
    "        graph_pyg = pyg.utils.from_networkx(graph_nx)\n",
    "\n",
    "        # transform graph if you like\n",
    "        graph_pyg = self.graph_transform(graph_pyg)\n",
    "\n",
    "        graph_pyg[\"file_name\"] = file_name\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    @staticmethod\n",
    "    def graph_transform(graph_pyg):\n",
    "        graph_pyg[\"room_type\"] = one_hot_encode(graph_pyg[\"room_type\"], NUM_ROOM_TYPES)\n",
    "\n",
    "        graph_pyg[\"zoning_type\"] = one_hot_encode(graph_pyg[\"zoning_type\"], NUM_ZONING_TYPES)\n",
    "\n",
    "        graph_pyg[\"connectivity\"] = one_hot_encode(graph_pyg[\"connectivity\"], len(CONNECTIVITIES))\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/path/to/modified-swiss-dwellings-train/\"\n",
    "\n",
    "# graph_nx = load_pickle(os.path.join(graph_path, f'{43}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = GraphZoningRoomTypeDataset(path, split=\"train\")\n",
    "\n",
    "ds_val = GraphZoningRoomTypeDataset(path, split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(ds_val)):\n",
    "    ds_val[index][\"n_corners\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shapely.geometry as sg\n",
    "\n",
    "\n",
    "\n",
    "# print(len(ds_train[0][\"polygon\"][14]))\n",
    "\n",
    "# display(sg.Polygon(ds_train[0][\"polygon\"][14]).simplify(tolerance=0.1, preserve_topology=True))\n",
    "\n",
    "\n",
    "# polygon = sg.Polygon(ds_train[0][\"polygon\"][14])\n",
    "# prev_len = 100000\n",
    "\n",
    "# while len(polygon.simplify(0.1).exterior.coords) < prev_len:\n",
    "#     polygon = polygon.simplify(0.1)\n",
    "#     prev_len = len(polygon.exterior.coords)\n",
    "\n",
    "# prev_len\n",
    "# # print(len(.simplify(.1).exterior.coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, num_features=NUM_ROOM_TYPES, num_edge_features=len(CONNECTIVITIES), hidden_size=32, target_size=1, num_hidden_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.target_size = target_size\n",
    "\n",
    "        self.convs = nn.ModuleList([GATConv(self.num_features if i == 0 else self.hidden_size, self.hidden_size, edge_dim=self.num_edge_features) for i in range(num_hidden_layers)])\n",
    "\n",
    "        if num_hidden_layers == 0:\n",
    "            self.hidden_linear = nn.Linear(self.num_features, self.hidden_size)\n",
    "        else:\n",
    "            self.hidden_linear = nn.Linear(self.hidden_size + num_features, self.hidden_size)\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.room_type, data.edge_index, data.connectivity\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr) # adding edge features here!\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "\n",
    "        # x = self.convs[-1](x, edge_index, edge_attr=edge_attr) # edge features here as well\n",
    "        \n",
    "        if len(self.convs) > 0:\n",
    "            x = torch.cat([x, data.room_type], dim=-1)\n",
    "\n",
    "        x = self.hidden_linear(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return F.relu(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "loss_func = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_val, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    accuracies = []\n",
    "\n",
    "    absolute_distances = []\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Batch size should be 1, because want to evaluate each graph separately\n",
    "        for data in DataLoader(data_val, batch_size=1):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "            pred = model(data.to(device)).squeeze()\n",
    "            \n",
    "            # pred = torch.argmax(pred, dim=-1)\n",
    "\n",
    "            gt = data.n_corners.squeeze()\n",
    "            \n",
    "            # gt = torch.argmax(data.n_corners, dim=-1).to(device)\n",
    "            \n",
    "            \n",
    "            # Tensor of shape (batch_size, 1)\n",
    "            acc = (torch.abs(gt - pred) < 1).sum(dim=-1) / pred.shape[-1]\n",
    "            acc = acc.item()\n",
    "            # acc = 0\n",
    "\n",
    "            absolute_distance = torch.abs(gt - pred).mean(dim=-1, dtype=torch.float32)\n",
    "            \n",
    "#             print(pred.shape, gt.shape)\n",
    "            \n",
    "#             print(absolute_distance.shape)\n",
    "\n",
    "            # pred_counter = Counter(pred.cpu().numpy())\n",
    "            # gt_counter = Counter(gt.cpu().numpy())\n",
    "\n",
    "            # iou = sum((pred_counter & gt_counter).values()) / sum((pred_counter | gt_counter).values())\n",
    "            # ious.append(iou)\n",
    "\n",
    "            accuracies.append(acc)\n",
    "\n",
    "            absolute_distances.append(absolute_distance.item())\n",
    "            \n",
    "            val_loss = loss_func(out, data.n_corners)\n",
    "\n",
    "            total_loss += val_loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return total_loss, np.mean(accuracies), np.mean(absolute_distances), np.std(absolute_distances)\n",
    "\n",
    "\n",
    "def train(model, data_train, data_val, batch_size, learning_rate, n_epochs=1, device=\"cpu\", save_loss_interval=1, print_interval=1):\n",
    "\n",
    "    NUM_TRAIN = len(data_train)\n",
    "    NUM_VAL = len(data_val)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # test_data = data_test[0]\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for data in loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(data)\n",
    "            \n",
    "            loss = loss_func(out, data.n_corners)\n",
    "            \n",
    "            epoch_loss += loss.item() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % save_loss_interval == 0:\n",
    "            val_loss, val_acc, mean_absolute_error, std_absolute_error = evaluate_model(model, data_val, device=\"cpu\")\n",
    "\n",
    "            val_loss /= NUM_VAL\n",
    "\n",
    "            model = model.to(device)\n",
    "\n",
    "            train_loss = epoch_loss / NUM_TRAIN\n",
    "          \n",
    "            if epoch % print_interval == 0:\n",
    "                print(f\"Epoch: {epoch} Train loss: {train_loss:.3e} Val loss: {val_loss:.3e} Val acc: {val_acc:.3f} Val mean absolute error: {mean_absolute_error:.3f} Val std absolute error: {std_absolute_error:.3f}\")\n",
    "          \n",
    "            yield {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_mean_absolute_error\": mean_absolute_error,\n",
    "                \"val_std_absolute_error\": std_absolute_error\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_with_early_stopping(model, ds_train, ds_val, batch_size=32, learning_rate=0.001, n_epochs=100, device=\"cuda\", tolerance=5):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    best_model = (None, 1e5, None)\n",
    "\n",
    "    for loss in train(model, ds_train, ds_val, batch_size=batch_size, learning_rate=learning_rate, n_epochs=n_epochs, device=device):\n",
    "        losses.append(loss)\n",
    "\n",
    "        if loss[\"val_loss\"] < best_model[1]:\n",
    "            best_model = (copy.deepcopy(model), loss[\"val_loss\"], loss)\n",
    "\n",
    "            print(f\"New best model with val loss: {loss['val_loss']:.3e}\")\n",
    "        \n",
    "        if loss[\"epoch\"] - best_model[2][\"epoch\"] > tolerance:\n",
    "            print(f\"Stopping early at epoch {loss['epoch']}\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"losses\": losses,\n",
    "        \"best_model\": best_model[0],\n",
    "        \"best_model_val_loss\": best_model[1],\n",
    "        \"best_model_loss_dict\": best_model[2],\n",
    "        \"last_model\": model,\n",
    "    }\n",
    "\n",
    "# train_with_early_stopping(model, ds_train, ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 3 hidden layers\n",
      "Epoch: 0 Train loss: 1.231e-01 Val loss: 1.796e+00 Val acc: 0.535 Val mean absolute error: 1.796 Val std absolute error: 0.745\n",
      "New best model with val loss: 1.796e+00\n",
      "Epoch: 1 Train loss: 7.445e-02 Val loss: 1.746e+00 Val acc: 0.511 Val mean absolute error: 1.746 Val std absolute error: 0.702\n",
      "New best model with val loss: 1.746e+00\n",
      "Epoch: 2 Train loss: 7.025e-02 Val loss: 1.712e+00 Val acc: 0.520 Val mean absolute error: 1.712 Val std absolute error: 0.693\n",
      "New best model with val loss: 1.712e+00\n",
      "Epoch: 3 Train loss: 6.742e-02 Val loss: 1.684e+00 Val acc: 0.539 Val mean absolute error: 1.684 Val std absolute error: 0.706\n",
      "New best model with val loss: 1.684e+00\n",
      "Epoch: 4 Train loss: 6.507e-02 Val loss: 1.673e+00 Val acc: 0.543 Val mean absolute error: 1.673 Val std absolute error: 0.720\n",
      "New best model with val loss: 1.673e+00\n",
      "Epoch: 5 Train loss: 6.384e-02 Val loss: 1.671e+00 Val acc: 0.539 Val mean absolute error: 1.671 Val std absolute error: 0.687\n",
      "New best model with val loss: 1.671e+00\n",
      "Epoch: 6 Train loss: 6.271e-02 Val loss: 1.669e+00 Val acc: 0.536 Val mean absolute error: 1.669 Val std absolute error: 0.675\n",
      "New best model with val loss: 1.669e+00\n",
      "Epoch: 7 Train loss: 6.226e-02 Val loss: 1.665e+00 Val acc: 0.544 Val mean absolute error: 1.665 Val std absolute error: 0.695\n",
      "New best model with val loss: 1.665e+00\n",
      "Epoch: 8 Train loss: 6.171e-02 Val loss: 1.666e+00 Val acc: 0.535 Val mean absolute error: 1.666 Val std absolute error: 0.677\n",
      "Epoch: 9 Train loss: 6.142e-02 Val loss: 1.662e+00 Val acc: 0.536 Val mean absolute error: 1.662 Val std absolute error: 0.677\n",
      "New best model with val loss: 1.662e+00\n",
      "Epoch: 10 Train loss: 6.104e-02 Val loss: 1.663e+00 Val acc: 0.536 Val mean absolute error: 1.663 Val std absolute error: 0.668\n",
      "Epoch: 11 Train loss: 6.077e-02 Val loss: 1.661e+00 Val acc: 0.536 Val mean absolute error: 1.661 Val std absolute error: 0.677\n",
      "New best model with val loss: 1.661e+00\n",
      "Epoch: 12 Train loss: 6.045e-02 Val loss: 1.661e+00 Val acc: 0.536 Val mean absolute error: 1.661 Val std absolute error: 0.676\n",
      "New best model with val loss: 1.661e+00\n",
      "Epoch: 13 Train loss: 6.016e-02 Val loss: 1.663e+00 Val acc: 0.536 Val mean absolute error: 1.663 Val std absolute error: 0.681\n",
      "Epoch: 14 Train loss: 5.997e-02 Val loss: 1.659e+00 Val acc: 0.536 Val mean absolute error: 1.659 Val std absolute error: 0.686\n",
      "New best model with val loss: 1.659e+00\n",
      "Epoch: 15 Train loss: 5.967e-02 Val loss: 1.659e+00 Val acc: 0.535 Val mean absolute error: 1.659 Val std absolute error: 0.693\n",
      "Epoch: 16 Train loss: 5.954e-02 Val loss: 1.659e+00 Val acc: 0.536 Val mean absolute error: 1.659 Val std absolute error: 0.681\n",
      "New best model with val loss: 1.659e+00\n",
      "Epoch: 17 Train loss: 5.911e-02 Val loss: 1.658e+00 Val acc: 0.533 Val mean absolute error: 1.658 Val std absolute error: 0.668\n",
      "New best model with val loss: 1.658e+00\n",
      "Epoch: 18 Train loss: 5.898e-02 Val loss: 1.655e+00 Val acc: 0.536 Val mean absolute error: 1.655 Val std absolute error: 0.691\n",
      "New best model with val loss: 1.655e+00\n",
      "Epoch: 19 Train loss: 5.868e-02 Val loss: 1.659e+00 Val acc: 0.536 Val mean absolute error: 1.659 Val std absolute error: 0.671\n",
      "Epoch: 20 Train loss: 5.838e-02 Val loss: 1.658e+00 Val acc: 0.536 Val mean absolute error: 1.658 Val std absolute error: 0.673\n",
      "Epoch: 21 Train loss: 5.801e-02 Val loss: 1.652e+00 Val acc: 0.536 Val mean absolute error: 1.652 Val std absolute error: 0.689\n",
      "New best model with val loss: 1.652e+00\n",
      "Epoch: 22 Train loss: 5.796e-02 Val loss: 1.654e+00 Val acc: 0.534 Val mean absolute error: 1.654 Val std absolute error: 0.672\n",
      "Epoch: 23 Train loss: 5.772e-02 Val loss: 1.652e+00 Val acc: 0.534 Val mean absolute error: 1.652 Val std absolute error: 0.679\n",
      "New best model with val loss: 1.652e+00\n",
      "Epoch: 24 Train loss: 5.731e-02 Val loss: 1.649e+00 Val acc: 0.532 Val mean absolute error: 1.649 Val std absolute error: 0.678\n",
      "New best model with val loss: 1.649e+00\n",
      "Epoch: 25 Train loss: 5.709e-02 Val loss: 1.650e+00 Val acc: 0.533 Val mean absolute error: 1.650 Val std absolute error: 0.679\n",
      "Epoch: 26 Train loss: 5.684e-02 Val loss: 1.642e+00 Val acc: 0.534 Val mean absolute error: 1.642 Val std absolute error: 0.692\n",
      "New best model with val loss: 1.642e+00\n",
      "Epoch: 27 Train loss: 5.640e-02 Val loss: 1.639e+00 Val acc: 0.534 Val mean absolute error: 1.639 Val std absolute error: 0.693\n",
      "New best model with val loss: 1.639e+00\n",
      "Epoch: 28 Train loss: 5.613e-02 Val loss: 1.637e+00 Val acc: 0.530 Val mean absolute error: 1.637 Val std absolute error: 0.662\n",
      "New best model with val loss: 1.637e+00\n",
      "Epoch: 29 Train loss: 5.589e-02 Val loss: 1.631e+00 Val acc: 0.536 Val mean absolute error: 1.631 Val std absolute error: 0.689\n",
      "New best model with val loss: 1.631e+00\n",
      "Epoch: 30 Train loss: 5.565e-02 Val loss: 1.627e+00 Val acc: 0.535 Val mean absolute error: 1.627 Val std absolute error: 0.682\n",
      "New best model with val loss: 1.627e+00\n",
      "Epoch: 31 Train loss: 5.535e-02 Val loss: 1.630e+00 Val acc: 0.537 Val mean absolute error: 1.630 Val std absolute error: 0.685\n",
      "Epoch: 32 Train loss: 5.512e-02 Val loss: 1.625e+00 Val acc: 0.537 Val mean absolute error: 1.625 Val std absolute error: 0.682\n",
      "New best model with val loss: 1.625e+00\n",
      "Epoch: 33 Train loss: 5.477e-02 Val loss: 1.622e+00 Val acc: 0.529 Val mean absolute error: 1.622 Val std absolute error: 0.669\n",
      "New best model with val loss: 1.622e+00\n",
      "Epoch: 34 Train loss: 5.450e-02 Val loss: 1.623e+00 Val acc: 0.539 Val mean absolute error: 1.623 Val std absolute error: 0.695\n",
      "Epoch: 35 Train loss: 5.453e-02 Val loss: 1.619e+00 Val acc: 0.531 Val mean absolute error: 1.619 Val std absolute error: 0.680\n",
      "New best model with val loss: 1.619e+00\n",
      "Epoch: 36 Train loss: 5.422e-02 Val loss: 1.621e+00 Val acc: 0.537 Val mean absolute error: 1.621 Val std absolute error: 0.689\n",
      "Epoch: 37 Train loss: 5.403e-02 Val loss: 1.624e+00 Val acc: 0.538 Val mean absolute error: 1.624 Val std absolute error: 0.689\n",
      "Epoch: 38 Train loss: 5.385e-02 Val loss: 1.618e+00 Val acc: 0.535 Val mean absolute error: 1.618 Val std absolute error: 0.683\n",
      "New best model with val loss: 1.618e+00\n",
      "Epoch: 39 Train loss: 5.354e-02 Val loss: 1.618e+00 Val acc: 0.529 Val mean absolute error: 1.618 Val std absolute error: 0.672\n",
      "New best model with val loss: 1.618e+00\n",
      "Epoch: 40 Train loss: 5.328e-02 Val loss: 1.621e+00 Val acc: 0.538 Val mean absolute error: 1.621 Val std absolute error: 0.688\n",
      "Epoch: 41 Train loss: 5.320e-02 Val loss: 1.615e+00 Val acc: 0.528 Val mean absolute error: 1.615 Val std absolute error: 0.668\n",
      "New best model with val loss: 1.615e+00\n",
      "Epoch: 42 Train loss: 5.286e-02 Val loss: 1.617e+00 Val acc: 0.538 Val mean absolute error: 1.617 Val std absolute error: 0.693\n",
      "Epoch: 43 Train loss: 5.287e-02 Val loss: 1.614e+00 Val acc: 0.533 Val mean absolute error: 1.614 Val std absolute error: 0.693\n",
      "New best model with val loss: 1.614e+00\n",
      "Epoch: 44 Train loss: 5.262e-02 Val loss: 1.615e+00 Val acc: 0.533 Val mean absolute error: 1.615 Val std absolute error: 0.691\n",
      "Epoch: 45 Train loss: 5.256e-02 Val loss: 1.614e+00 Val acc: 0.534 Val mean absolute error: 1.614 Val std absolute error: 0.686\n",
      "New best model with val loss: 1.614e+00\n",
      "Epoch: 46 Train loss: 5.225e-02 Val loss: 1.613e+00 Val acc: 0.538 Val mean absolute error: 1.613 Val std absolute error: 0.708\n",
      "New best model with val loss: 1.613e+00\n",
      "Epoch: 47 Train loss: 5.215e-02 Val loss: 1.616e+00 Val acc: 0.543 Val mean absolute error: 1.616 Val std absolute error: 0.710\n",
      "Epoch: 48 Train loss: 5.211e-02 Val loss: 1.609e+00 Val acc: 0.532 Val mean absolute error: 1.609 Val std absolute error: 0.686\n",
      "New best model with val loss: 1.609e+00\n",
      "Epoch: 49 Train loss: 5.206e-02 Val loss: 1.612e+00 Val acc: 0.536 Val mean absolute error: 1.612 Val std absolute error: 0.698\n",
      "Epoch: 50 Train loss: 5.183e-02 Val loss: 1.609e+00 Val acc: 0.536 Val mean absolute error: 1.609 Val std absolute error: 0.695\n",
      "Epoch: 51 Train loss: 5.170e-02 Val loss: 1.605e+00 Val acc: 0.530 Val mean absolute error: 1.605 Val std absolute error: 0.692\n",
      "New best model with val loss: 1.605e+00\n",
      "Epoch: 52 Train loss: 5.148e-02 Val loss: 1.603e+00 Val acc: 0.529 Val mean absolute error: 1.603 Val std absolute error: 0.681\n",
      "New best model with val loss: 1.603e+00\n",
      "Epoch: 53 Train loss: 5.163e-02 Val loss: 1.604e+00 Val acc: 0.531 Val mean absolute error: 1.604 Val std absolute error: 0.686\n",
      "Epoch: 54 Train loss: 5.127e-02 Val loss: 1.609e+00 Val acc: 0.534 Val mean absolute error: 1.609 Val std absolute error: 0.710\n",
      "Epoch: 55 Train loss: 5.113e-02 Val loss: 1.602e+00 Val acc: 0.531 Val mean absolute error: 1.602 Val std absolute error: 0.698\n",
      "New best model with val loss: 1.602e+00\n",
      "Epoch: 56 Train loss: 5.110e-02 Val loss: 1.603e+00 Val acc: 0.533 Val mean absolute error: 1.603 Val std absolute error: 0.706\n",
      "Epoch: 57 Train loss: 5.092e-02 Val loss: 1.604e+00 Val acc: 0.535 Val mean absolute error: 1.604 Val std absolute error: 0.703\n",
      "Epoch: 58 Train loss: 5.081e-02 Val loss: 1.600e+00 Val acc: 0.529 Val mean absolute error: 1.600 Val std absolute error: 0.691\n",
      "New best model with val loss: 1.600e+00\n",
      "Epoch: 59 Train loss: 5.082e-02 Val loss: 1.594e+00 Val acc: 0.527 Val mean absolute error: 1.594 Val std absolute error: 0.689\n",
      "New best model with val loss: 1.594e+00\n",
      "Epoch: 60 Train loss: 5.065e-02 Val loss: 1.600e+00 Val acc: 0.534 Val mean absolute error: 1.600 Val std absolute error: 0.706\n",
      "Epoch: 61 Train loss: 5.064e-02 Val loss: 1.601e+00 Val acc: 0.532 Val mean absolute error: 1.601 Val std absolute error: 0.713\n",
      "Epoch: 62 Train loss: 5.038e-02 Val loss: 1.599e+00 Val acc: 0.532 Val mean absolute error: 1.599 Val std absolute error: 0.710\n",
      "Epoch: 63 Train loss: 5.043e-02 Val loss: 1.602e+00 Val acc: 0.537 Val mean absolute error: 1.602 Val std absolute error: 0.721\n",
      "Epoch: 64 Train loss: 5.030e-02 Val loss: 1.600e+00 Val acc: 0.533 Val mean absolute error: 1.600 Val std absolute error: 0.710\n",
      "Epoch: 65 Train loss: 5.028e-02 Val loss: 1.597e+00 Val acc: 0.531 Val mean absolute error: 1.597 Val std absolute error: 0.715\n",
      "Stopping early at epoch 65\n"
     ]
    }
   ],
   "source": [
    "for num_hidden_layers in [3]:\n",
    "    model = GATModel(num_hidden_layers=num_hidden_layers)\n",
    "\n",
    "    print(f\"Training model with {num_hidden_layers} hidden layers\")\n",
    "\n",
    "    result = train_with_early_stopping(model, ds_train, ds_val, batch_size=32, learning_rate=0.001, n_epochs=100, device=\"cuda\", tolerance=5)\n",
    "\n",
    "    torch.save(result, f\"n_corners_classifier_mse_early_stopping_results_{num_hidden_layers}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to train longer\n",
    "\n",
    "Train longer with increasing batch size, gets slightly better results. However the accuracy, and std absolute error are still similar to above that trained only once.\n",
    "\n",
    "It seems predicting the number of corners per room is a lot harder than predicting room_type. This would also make sense, as the number of corners would depend a lot more on the structural walls compared to the room_type, but structural walls isn't given as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 9.680e-02 Val loss: 1.542e+00 Val acc: 0.509 Val mean absolute error: 1.542 Val std absolute error: 0.720\n",
      "New best model with val loss: 1.542e+00\n",
      "Epoch: 1 Train loss: 9.715e-02 Val loss: 1.543e+00 Val acc: 0.568 Val mean absolute error: 1.543 Val std absolute error: 0.723\n",
      "Epoch: 2 Train loss: 9.662e-02 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.719\n",
      "New best model with val loss: 1.542e+00\n",
      "Epoch: 3 Train loss: 9.691e-02 Val loss: 1.544e+00 Val acc: 0.566 Val mean absolute error: 1.544 Val std absolute error: 0.722\n",
      "Epoch: 4 Train loss: 9.722e-02 Val loss: 1.543e+00 Val acc: 0.565 Val mean absolute error: 1.543 Val std absolute error: 0.722\n",
      "Epoch: 5 Train loss: 9.694e-02 Val loss: 1.540e+00 Val acc: 0.564 Val mean absolute error: 1.540 Val std absolute error: 0.717\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 6 Train loss: 9.698e-02 Val loss: 1.542e+00 Val acc: 0.566 Val mean absolute error: 1.542 Val std absolute error: 0.719\n",
      "Epoch: 7 Train loss: 9.757e-02 Val loss: 1.542e+00 Val acc: 0.567 Val mean absolute error: 1.542 Val std absolute error: 0.717\n",
      "Epoch: 8 Train loss: 9.723e-02 Val loss: 1.545e+00 Val acc: 0.567 Val mean absolute error: 1.545 Val std absolute error: 0.728\n",
      "Epoch: 9 Train loss: 9.713e-02 Val loss: 1.542e+00 Val acc: 0.564 Val mean absolute error: 1.542 Val std absolute error: 0.712\n",
      "Epoch: 10 Train loss: 9.711e-02 Val loss: 1.545e+00 Val acc: 0.565 Val mean absolute error: 1.545 Val std absolute error: 0.728\n",
      "Epoch: 11 Train loss: 9.703e-02 Val loss: 1.542e+00 Val acc: 0.564 Val mean absolute error: 1.542 Val std absolute error: 0.716\n",
      "Epoch: 12 Train loss: 9.713e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.720\n",
      "Epoch: 13 Train loss: 9.703e-02 Val loss: 1.543e+00 Val acc: 0.565 Val mean absolute error: 1.543 Val std absolute error: 0.718\n",
      "Epoch: 14 Train loss: 9.715e-02 Val loss: 1.545e+00 Val acc: 0.568 Val mean absolute error: 1.545 Val std absolute error: 0.729\n",
      "Epoch: 15 Train loss: 9.731e-02 Val loss: 1.542e+00 Val acc: 0.564 Val mean absolute error: 1.542 Val std absolute error: 0.719\n",
      "Epoch: 16 Train loss: 9.691e-02 Val loss: 1.539e+00 Val acc: 0.565 Val mean absolute error: 1.539 Val std absolute error: 0.716\n",
      "New best model with val loss: 1.539e+00\n",
      "Epoch: 17 Train loss: 9.696e-02 Val loss: 1.540e+00 Val acc: 0.564 Val mean absolute error: 1.540 Val std absolute error: 0.721\n",
      "Epoch: 18 Train loss: 9.694e-02 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.722\n",
      "Epoch: 19 Train loss: 9.700e-02 Val loss: 1.543e+00 Val acc: 0.512 Val mean absolute error: 1.543 Val std absolute error: 0.728\n",
      "Epoch: 20 Train loss: 9.701e-02 Val loss: 1.545e+00 Val acc: 0.509 Val mean absolute error: 1.545 Val std absolute error: 0.726\n",
      "Epoch: 21 Train loss: 9.706e-02 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 22 Train loss: 9.687e-02 Val loss: 1.547e+00 Val acc: 0.567 Val mean absolute error: 1.547 Val std absolute error: 0.727\n",
      "Epoch: 23 Train loss: 9.699e-02 Val loss: 1.542e+00 Val acc: 0.567 Val mean absolute error: 1.542 Val std absolute error: 0.725\n",
      "Epoch: 24 Train loss: 9.735e-02 Val loss: 1.544e+00 Val acc: 0.511 Val mean absolute error: 1.544 Val std absolute error: 0.729\n",
      "Epoch: 25 Train loss: 9.700e-02 Val loss: 1.545e+00 Val acc: 0.509 Val mean absolute error: 1.545 Val std absolute error: 0.729\n",
      "Epoch: 26 Train loss: 9.696e-02 Val loss: 1.544e+00 Val acc: 0.511 Val mean absolute error: 1.544 Val std absolute error: 0.716\n",
      "Epoch: 27 Train loss: 9.712e-02 Val loss: 1.543e+00 Val acc: 0.564 Val mean absolute error: 1.543 Val std absolute error: 0.725\n",
      "Stopping early at epoch 27\n",
      "Epoch: 0 Train loss: 4.840e-02 Val loss: 1.539e+00 Val acc: 0.564 Val mean absolute error: 1.539 Val std absolute error: 0.721\n",
      "New best model with val loss: 1.539e+00\n",
      "Epoch: 1 Train loss: 4.837e-02 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.725\n",
      "Epoch: 2 Train loss: 4.850e-02 Val loss: 1.542e+00 Val acc: 0.509 Val mean absolute error: 1.542 Val std absolute error: 0.719\n",
      "Epoch: 3 Train loss: 4.847e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.721\n",
      "Epoch: 4 Train loss: 4.855e-02 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.719\n",
      "Epoch: 5 Train loss: 4.828e-02 Val loss: 1.542e+00 Val acc: 0.566 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 6 Train loss: 4.832e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.720\n",
      "Epoch: 7 Train loss: 4.846e-02 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.727\n",
      "Epoch: 8 Train loss: 4.843e-02 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.715\n",
      "Epoch: 9 Train loss: 4.839e-02 Val loss: 1.545e+00 Val acc: 0.565 Val mean absolute error: 1.545 Val std absolute error: 0.725\n",
      "Epoch: 10 Train loss: 4.845e-02 Val loss: 1.544e+00 Val acc: 0.509 Val mean absolute error: 1.544 Val std absolute error: 0.727\n",
      "Epoch: 11 Train loss: 4.846e-02 Val loss: 1.542e+00 Val acc: 0.566 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Stopping early at epoch 11\n",
      "Epoch: 0 Train loss: 2.425e-02 Val loss: 1.540e+00 Val acc: 0.566 Val mean absolute error: 1.540 Val std absolute error: 0.716\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 1 Train loss: 2.420e-02 Val loss: 1.542e+00 Val acc: 0.567 Val mean absolute error: 1.542 Val std absolute error: 0.727\n",
      "Epoch: 2 Train loss: 2.421e-02 Val loss: 1.544e+00 Val acc: 0.567 Val mean absolute error: 1.544 Val std absolute error: 0.724\n",
      "Epoch: 3 Train loss: 2.421e-02 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 4 Train loss: 2.419e-02 Val loss: 1.541e+00 Val acc: 0.564 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 5 Train loss: 2.423e-02 Val loss: 1.543e+00 Val acc: 0.567 Val mean absolute error: 1.543 Val std absolute error: 0.728\n",
      "Epoch: 6 Train loss: 2.422e-02 Val loss: 1.541e+00 Val acc: 0.509 Val mean absolute error: 1.541 Val std absolute error: 0.719\n",
      "Epoch: 7 Train loss: 2.420e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 8 Train loss: 2.426e-02 Val loss: 1.543e+00 Val acc: 0.566 Val mean absolute error: 1.543 Val std absolute error: 0.725\n",
      "Epoch: 9 Train loss: 2.418e-02 Val loss: 1.540e+00 Val acc: 0.564 Val mean absolute error: 1.540 Val std absolute error: 0.721\n",
      "Epoch: 10 Train loss: 2.424e-02 Val loss: 1.542e+00 Val acc: 0.511 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 11 Train loss: 2.421e-02 Val loss: 1.543e+00 Val acc: 0.511 Val mean absolute error: 1.543 Val std absolute error: 0.722\n",
      "Stopping early at epoch 11\n",
      "Epoch: 0 Train loss: 1.208e-02 Val loss: 1.543e+00 Val acc: 0.566 Val mean absolute error: 1.543 Val std absolute error: 0.727\n",
      "New best model with val loss: 1.543e+00\n",
      "Epoch: 1 Train loss: 1.209e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.721\n",
      "New best model with val loss: 1.541e+00\n",
      "Epoch: 2 Train loss: 1.210e-02 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.723\n",
      "Epoch: 3 Train loss: 1.208e-02 Val loss: 1.541e+00 Val acc: 0.509 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "New best model with val loss: 1.541e+00\n",
      "Epoch: 4 Train loss: 1.209e-02 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.721\n",
      "Epoch: 5 Train loss: 1.209e-02 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 6 Train loss: 1.210e-02 Val loss: 1.542e+00 Val acc: 0.567 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 7 Train loss: 1.210e-02 Val loss: 1.540e+00 Val acc: 0.564 Val mean absolute error: 1.540 Val std absolute error: 0.720\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 8 Train loss: 1.208e-02 Val loss: 1.544e+00 Val acc: 0.566 Val mean absolute error: 1.544 Val std absolute error: 0.727\n",
      "Epoch: 9 Train loss: 1.209e-02 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 10 Train loss: 1.211e-02 Val loss: 1.541e+00 Val acc: 0.564 Val mean absolute error: 1.541 Val std absolute error: 0.721\n",
      "Epoch: 11 Train loss: 1.209e-02 Val loss: 1.541e+00 Val acc: 0.510 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 12 Train loss: 1.207e-02 Val loss: 1.540e+00 Val acc: 0.564 Val mean absolute error: 1.540 Val std absolute error: 0.720\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 13 Train loss: 1.208e-02 Val loss: 1.542e+00 Val acc: 0.566 Val mean absolute error: 1.542 Val std absolute error: 0.725\n",
      "Epoch: 14 Train loss: 1.209e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 15 Train loss: 1.209e-02 Val loss: 1.542e+00 Val acc: 0.510 Val mean absolute error: 1.542 Val std absolute error: 0.725\n",
      "Epoch: 16 Train loss: 1.210e-02 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 17 Train loss: 1.209e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.721\n",
      "Epoch: 18 Train loss: 1.210e-02 Val loss: 1.540e+00 Val acc: 0.564 Val mean absolute error: 1.540 Val std absolute error: 0.723\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 19 Train loss: 1.209e-02 Val loss: 1.542e+00 Val acc: 0.509 Val mean absolute error: 1.542 Val std absolute error: 0.722\n",
      "Epoch: 20 Train loss: 1.209e-02 Val loss: 1.541e+00 Val acc: 0.564 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 21 Train loss: 1.210e-02 Val loss: 1.541e+00 Val acc: 0.564 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 22 Train loss: 1.208e-02 Val loss: 1.542e+00 Val acc: 0.509 Val mean absolute error: 1.542 Val std absolute error: 0.725\n",
      "Epoch: 23 Train loss: 1.208e-02 Val loss: 1.543e+00 Val acc: 0.509 Val mean absolute error: 1.543 Val std absolute error: 0.727\n",
      "Epoch: 24 Train loss: 1.210e-02 Val loss: 1.541e+00 Val acc: 0.564 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 25 Train loss: 1.210e-02 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.721\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 26 Train loss: 1.208e-02 Val loss: 1.542e+00 Val acc: 0.566 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 27 Train loss: 1.211e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 28 Train loss: 1.209e-02 Val loss: 1.542e+00 Val acc: 0.513 Val mean absolute error: 1.542 Val std absolute error: 0.723\n",
      "Epoch: 29 Train loss: 1.208e-02 Val loss: 1.541e+00 Val acc: 0.564 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 30 Train loss: 1.209e-02 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.721\n",
      "Epoch: 31 Train loss: 1.211e-02 Val loss: 1.541e+00 Val acc: 0.511 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 32 Train loss: 1.210e-02 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.719\n",
      "Epoch: 33 Train loss: 1.209e-02 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.722\n",
      "Epoch: 34 Train loss: 1.209e-02 Val loss: 1.542e+00 Val acc: 0.567 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 35 Train loss: 1.209e-02 Val loss: 1.542e+00 Val acc: 0.564 Val mean absolute error: 1.542 Val std absolute error: 0.723\n",
      "Epoch: 36 Train loss: 1.211e-02 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.721\n",
      "Stopping early at epoch 36\n",
      "Epoch: 0 Train loss: 6.263e-03 Val loss: 1.542e+00 Val acc: 0.565 Val mean absolute error: 1.542 Val std absolute error: 0.725\n",
      "New best model with val loss: 1.542e+00\n",
      "Epoch: 1 Train loss: 6.253e-03 Val loss: 1.540e+00 Val acc: 0.566 Val mean absolute error: 1.540 Val std absolute error: 0.721\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 2 Train loss: 6.232e-03 Val loss: 1.542e+00 Val acc: 0.566 Val mean absolute error: 1.542 Val std absolute error: 0.722\n",
      "Epoch: 3 Train loss: 6.211e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.721\n",
      "Epoch: 4 Train loss: 6.255e-03 Val loss: 1.541e+00 Val acc: 0.509 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 5 Train loss: 6.250e-03 Val loss: 1.542e+00 Val acc: 0.567 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 6 Train loss: 6.228e-03 Val loss: 1.541e+00 Val acc: 0.510 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 7 Train loss: 6.265e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 8 Train loss: 6.254e-03 Val loss: 1.541e+00 Val acc: 0.511 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 9 Train loss: 6.255e-03 Val loss: 1.541e+00 Val acc: 0.511 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 10 Train loss: 6.229e-03 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.721\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 11 Train loss: 6.253e-03 Val loss: 1.541e+00 Val acc: 0.564 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 12 Train loss: 6.252e-03 Val loss: 1.541e+00 Val acc: 0.509 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 13 Train loss: 6.244e-03 Val loss: 1.541e+00 Val acc: 0.567 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 14 Train loss: 6.223e-03 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 15 Train loss: 6.241e-03 Val loss: 1.541e+00 Val acc: 0.509 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 16 Train loss: 6.229e-03 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.722\n",
      "Epoch: 17 Train loss: 6.231e-03 Val loss: 1.542e+00 Val acc: 0.566 Val mean absolute error: 1.542 Val std absolute error: 0.723\n",
      "Epoch: 18 Train loss: 6.232e-03 Val loss: 1.541e+00 Val acc: 0.564 Val mean absolute error: 1.541 Val std absolute error: 0.720\n",
      "Epoch: 19 Train loss: 6.248e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 20 Train loss: 6.223e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 21 Train loss: 6.247e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Stopping early at epoch 21\n",
      "Epoch: 0 Train loss: 3.120e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "New best model with val loss: 1.541e+00\n",
      "Epoch: 1 Train loss: 3.119e-03 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 2 Train loss: 3.127e-03 Val loss: 1.542e+00 Val acc: 0.567 Val mean absolute error: 1.542 Val std absolute error: 0.724\n",
      "Epoch: 3 Train loss: 3.118e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.721\n",
      "New best model with val loss: 1.541e+00\n",
      "Epoch: 4 Train loss: 3.117e-03 Val loss: 1.541e+00 Val acc: 0.567 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 5 Train loss: 3.113e-03 Val loss: 1.540e+00 Val acc: 0.566 Val mean absolute error: 1.540 Val std absolute error: 0.722\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 6 Train loss: 3.112e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 7 Train loss: 3.119e-03 Val loss: 1.542e+00 Val acc: 0.566 Val mean absolute error: 1.542 Val std absolute error: 0.723\n",
      "Epoch: 8 Train loss: 3.121e-03 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.725\n",
      "Epoch: 9 Train loss: 3.120e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 10 Train loss: 3.116e-03 Val loss: 1.540e+00 Val acc: 0.509 Val mean absolute error: 1.540 Val std absolute error: 0.721\n",
      "Epoch: 11 Train loss: 3.112e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 12 Train loss: 3.118e-03 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.723\n",
      "Epoch: 13 Train loss: 3.121e-03 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.725\n",
      "Epoch: 14 Train loss: 3.113e-03 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.720\n",
      "New best model with val loss: 1.540e+00\n",
      "Epoch: 15 Train loss: 3.113e-03 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.722\n",
      "Epoch: 16 Train loss: 3.123e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 17 Train loss: 3.120e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 18 Train loss: 3.124e-03 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 19 Train loss: 3.118e-03 Val loss: 1.541e+00 Val acc: 0.509 Val mean absolute error: 1.541 Val std absolute error: 0.722\n",
      "Epoch: 20 Train loss: 3.116e-03 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.723\n",
      "Epoch: 21 Train loss: 3.114e-03 Val loss: 1.540e+00 Val acc: 0.566 Val mean absolute error: 1.540 Val std absolute error: 0.722\n",
      "Epoch: 22 Train loss: 3.113e-03 Val loss: 1.541e+00 Val acc: 0.566 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Epoch: 23 Train loss: 3.118e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.724\n",
      "Epoch: 24 Train loss: 3.109e-03 Val loss: 1.540e+00 Val acc: 0.565 Val mean absolute error: 1.540 Val std absolute error: 0.721\n",
      "Epoch: 25 Train loss: 3.120e-03 Val loss: 1.541e+00 Val acc: 0.565 Val mean absolute error: 1.541 Val std absolute error: 0.723\n",
      "Stopping early at epoch 25\n"
     ]
    }
   ],
   "source": [
    "# model = GATModel(num_hidden_layers=1)\n",
    "\n",
    "for batch_size in [16, 32, 64, 128, 256, 512]:\n",
    "    result = train_with_early_stopping(model, ds_train, ds_val, batch_size=batch_size, learning_rate=0.0005, n_epochs=400, device=\"cuda\", tolerance=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dict = result[\"best_model_loss_dict\"]\n",
    "\n",
    "model = result[\"best_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(result, \"n_corners_classifier_mse_early_stopping_results:best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6633778779130233 0.5360361379370735\n",
      "1.5562544480750435 0.5636744407945843\n",
      "1.5851912213284434 0.5406600720645708\n",
      "1.594444824748062 0.5266760762727432\n",
      "1.6629817731072458 0.5364675197090829\n",
      "1.6646894507430958 0.5364675197090829\n",
      "1.663627598559457 0.5360361379370735\n"
     ]
    }
   ],
   "source": [
    "val_loss_dicts = []\n",
    "\n",
    "for num_hidden_layers in [0, 1, 2, 3, 4, 8, 16]:\n",
    "    result_i = torch.load( f\"n_corners_classifier_mse_early_stopping_results_{num_hidden_layers}.pt\", map_location=\"cpu\")\n",
    "\n",
    "    print(result_i[\"best_model_val_loss\"], result_i[\"best_model_loss_dict\"][\"val_acc\"])\n",
    "\n",
    "    val_loss_dicts.append({\n",
    "        \"num_hidden_layers\": num_hidden_layers,\n",
    "        \"val_loss\": result_i[\"best_model_val_loss\"],\n",
    "        # \"max_epoch\": result_i[\"losses\"][-1][\"epoch\"],\n",
    "        \"val_acc\": result_i[\"best_model_loss_dict\"][\"val_acc\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['losses', 'best_model', 'best_model_val_loss', 'best_model_loss_dict', 'last_model'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result_i = torch.load( f\"n_corners_classifier_mse_early_stopping_results_{1}.pt\", map_location=\"cpu\")\n",
    "\n",
    "result_i = torch.load(\"n_corners_classifier_mse_early_stopping_results:best.pt\", map_location=\"cpu\")\n",
    "\n",
    "result_i.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATModel(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(9, 32, heads=1)\n",
       "  )\n",
       "  (hidden_linear): Linear(in_features=41, out_features=32, bias=True)\n",
       "  (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_i[\"best_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# pd.DataFrame(val_loss_dicts).plot.bar(x=\"num_hidden_layers\", y=\"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrr}\n",
      "num hidden layers & val loss & val acc \\\\\n",
      "2 & 0.35 & 0.87 \\\\\n",
      "3 & 0.30 & 0.90 \\\\\n",
      "4 & 0.35 & 0.87 \\\\\n",
      "8 & 0.41 & 0.83 \\\\\n",
      "16 & 0.58 & 0.74 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(val_loss_dicts)[[\"num_hidden_layers\", \"val_loss\", \"val_acc\"]].style.hide(axis=\"index\").format(lambda x: f\"{x:.2f}\", [\"val_loss\", \"val_acc\"]).to_latex().replace(\"_\", \" \"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_hidden_layers = 3\n",
    "# results_dict = torch.load(f\"room_type_classifier_early_stopping_results_{num_hidden_layers}.pt\")\n",
    "\n",
    "# model = results_dict[\"best_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATModel(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(9, 32, heads=1)\n",
       "  )\n",
       "  (hidden_linear): Linear(in_features=41, out_features=32, bias=True)\n",
       "  (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 80], zoning_type=[38, 4], room_type=[38, 9], connectivity=[80, 3], num_nodes=38, file_name='4167.pickle')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test = GraphZoningTypeTestSet('/path/to/modified-swiss-dwellings-v1-test/', \"graph_pred\")\n",
    "\n",
    "ds_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GraphZoningTypeTestSet at 0x7f2340c57190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_graph_path = ds_test.graph_path.replace(\"graph_pred\", \"graph_pred_n_corners\")\n",
    "\n",
    "os.makedirs(pred_graph_path, exist_ok=True)\n",
    "\n",
    "pred_graph_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4167.pickle\n",
      "4168.pickle\n",
      "4169.pickle\n",
      "4170.pickle\n",
      "4171.pickle\n",
      "4172.pickle\n",
      "4173.pickle\n",
      "4174.pickle\n",
      "4175.pickle\n",
      "4176.pickle\n",
      "4177.pickle\n",
      "4178.pickle\n",
      "4179.pickle\n",
      "4180.pickle\n",
      "4181.pickle\n",
      "4182.pickle\n",
      "4183.pickle\n",
      "4184.pickle\n",
      "4185.pickle\n",
      "4186.pickle\n",
      "4187.pickle\n",
      "4188.pickle\n",
      "4189.pickle\n",
      "4190.pickle\n",
      "4191.pickle\n",
      "4192.pickle\n",
      "4193.pickle\n",
      "4194.pickle\n",
      "4195.pickle\n",
      "4196.pickle\n",
      "4197.pickle\n",
      "4198.pickle\n",
      "4199.pickle\n",
      "4200.pickle\n",
      "4201.pickle\n",
      "4202.pickle\n",
      "4203.pickle\n",
      "4204.pickle\n",
      "4205.pickle\n",
      "4206.pickle\n",
      "4207.pickle\n",
      "4208.pickle\n",
      "4209.pickle\n",
      "4210.pickle\n",
      "4211.pickle\n",
      "4212.pickle\n",
      "4213.pickle\n",
      "4214.pickle\n",
      "4215.pickle\n",
      "4216.pickle\n",
      "4217.pickle\n",
      "4218.pickle\n",
      "4219.pickle\n",
      "4220.pickle\n",
      "4221.pickle\n",
      "4222.pickle\n",
      "4223.pickle\n",
      "4224.pickle\n",
      "4225.pickle\n",
      "4226.pickle\n",
      "4227.pickle\n",
      "4228.pickle\n",
      "4229.pickle\n",
      "4230.pickle\n",
      "4231.pickle\n",
      "4232.pickle\n",
      "4233.pickle\n",
      "4234.pickle\n",
      "4235.pickle\n",
      "4236.pickle\n",
      "4237.pickle\n",
      "4238.pickle\n",
      "4239.pickle\n",
      "4240.pickle\n",
      "4241.pickle\n",
      "4242.pickle\n",
      "4243.pickle\n",
      "4244.pickle\n",
      "4245.pickle\n",
      "4246.pickle\n",
      "4247.pickle\n",
      "4248.pickle\n",
      "4249.pickle\n",
      "4250.pickle\n",
      "4251.pickle\n",
      "4252.pickle\n",
      "4253.pickle\n",
      "4254.pickle\n",
      "4255.pickle\n",
      "4256.pickle\n",
      "4257.pickle\n",
      "4258.pickle\n",
      "4259.pickle\n",
      "4260.pickle\n",
      "4261.pickle\n",
      "4262.pickle\n",
      "4263.pickle\n",
      "4264.pickle\n",
      "4265.pickle\n",
      "4266.pickle\n",
      "4267.pickle\n",
      "4268.pickle\n",
      "4269.pickle\n",
      "4270.pickle\n",
      "4271.pickle\n",
      "4272.pickle\n",
      "4273.pickle\n",
      "4274.pickle\n",
      "4275.pickle\n",
      "4276.pickle\n",
      "4277.pickle\n",
      "4278.pickle\n",
      "4279.pickle\n",
      "4280.pickle\n",
      "4281.pickle\n",
      "4282.pickle\n",
      "4283.pickle\n",
      "4284.pickle\n",
      "4285.pickle\n",
      "4286.pickle\n",
      "4287.pickle\n",
      "4288.pickle\n",
      "4289.pickle\n",
      "4290.pickle\n",
      "4291.pickle\n",
      "4292.pickle\n",
      "4293.pickle\n",
      "4294.pickle\n",
      "4295.pickle\n",
      "4296.pickle\n",
      "4297.pickle\n",
      "4298.pickle\n",
      "4299.pickle\n",
      "4300.pickle\n",
      "4301.pickle\n",
      "4302.pickle\n",
      "4303.pickle\n",
      "4304.pickle\n",
      "4305.pickle\n",
      "4306.pickle\n",
      "4307.pickle\n",
      "4308.pickle\n",
      "4309.pickle\n",
      "4310.pickle\n",
      "4311.pickle\n",
      "4312.pickle\n",
      "4313.pickle\n",
      "4314.pickle\n",
      "4315.pickle\n",
      "4316.pickle\n",
      "4317.pickle\n",
      "4318.pickle\n",
      "4319.pickle\n",
      "4320.pickle\n",
      "4321.pickle\n",
      "4322.pickle\n",
      "4323.pickle\n",
      "4324.pickle\n",
      "4325.pickle\n",
      "4326.pickle\n",
      "4327.pickle\n",
      "4328.pickle\n",
      "4329.pickle\n",
      "4330.pickle\n",
      "4331.pickle\n",
      "4332.pickle\n",
      "4333.pickle\n",
      "4334.pickle\n",
      "4335.pickle\n",
      "4336.pickle\n",
      "4337.pickle\n",
      "4338.pickle\n",
      "4339.pickle\n",
      "4340.pickle\n",
      "4341.pickle\n",
      "4342.pickle\n",
      "4343.pickle\n",
      "4344.pickle\n",
      "4345.pickle\n",
      "4346.pickle\n",
      "4347.pickle\n",
      "4348.pickle\n",
      "4349.pickle\n",
      "4350.pickle\n",
      "4351.pickle\n",
      "4352.pickle\n",
      "4353.pickle\n",
      "4354.pickle\n",
      "4355.pickle\n",
      "4356.pickle\n",
      "4357.pickle\n",
      "4358.pickle\n",
      "4359.pickle\n",
      "4360.pickle\n",
      "4361.pickle\n",
      "4362.pickle\n",
      "4363.pickle\n",
      "4364.pickle\n",
      "4365.pickle\n",
      "4366.pickle\n",
      "4367.pickle\n",
      "4368.pickle\n",
      "4369.pickle\n",
      "4370.pickle\n",
      "4371.pickle\n",
      "4372.pickle\n",
      "4373.pickle\n",
      "4374.pickle\n",
      "4375.pickle\n",
      "4376.pickle\n",
      "4377.pickle\n",
      "4378.pickle\n",
      "4379.pickle\n",
      "4380.pickle\n",
      "4381.pickle\n",
      "4382.pickle\n",
      "4383.pickle\n",
      "4384.pickle\n",
      "4385.pickle\n",
      "4386.pickle\n",
      "4387.pickle\n",
      "4388.pickle\n",
      "4389.pickle\n",
      "4390.pickle\n",
      "4391.pickle\n",
      "4392.pickle\n",
      "4393.pickle\n",
      "4394.pickle\n",
      "4395.pickle\n",
      "4396.pickle\n",
      "4397.pickle\n",
      "4398.pickle\n",
      "4399.pickle\n",
      "4400.pickle\n",
      "4401.pickle\n",
      "4402.pickle\n",
      "4403.pickle\n",
      "4404.pickle\n",
      "4405.pickle\n",
      "4406.pickle\n",
      "4407.pickle\n",
      "4408.pickle\n",
      "4409.pickle\n",
      "4410.pickle\n",
      "4411.pickle\n",
      "4412.pickle\n",
      "4413.pickle\n",
      "4414.pickle\n",
      "4415.pickle\n",
      "4416.pickle\n",
      "4417.pickle\n",
      "4418.pickle\n",
      "4419.pickle\n",
      "4420.pickle\n",
      "4421.pickle\n",
      "4422.pickle\n",
      "4423.pickle\n",
      "4424.pickle\n",
      "4425.pickle\n",
      "4426.pickle\n",
      "4427.pickle\n",
      "4428.pickle\n",
      "4429.pickle\n",
      "4430.pickle\n",
      "4431.pickle\n",
      "4432.pickle\n",
      "4433.pickle\n",
      "4434.pickle\n",
      "4435.pickle\n",
      "4436.pickle\n",
      "4437.pickle\n",
      "4438.pickle\n",
      "4439.pickle\n",
      "4440.pickle\n",
      "4441.pickle\n",
      "4442.pickle\n",
      "4443.pickle\n",
      "4444.pickle\n",
      "4445.pickle\n",
      "4446.pickle\n",
      "4447.pickle\n",
      "4448.pickle\n",
      "4449.pickle\n",
      "4450.pickle\n",
      "4451.pickle\n",
      "4452.pickle\n",
      "4453.pickle\n",
      "4454.pickle\n",
      "4455.pickle\n",
      "4456.pickle\n",
      "4457.pickle\n",
      "4458.pickle\n",
      "4459.pickle\n",
      "4460.pickle\n",
      "4461.pickle\n",
      "4462.pickle\n",
      "4463.pickle\n",
      "4464.pickle\n",
      "4465.pickle\n",
      "4466.pickle\n",
      "4467.pickle\n",
      "4468.pickle\n",
      "4469.pickle\n",
      "4470.pickle\n",
      "4471.pickle\n",
      "4472.pickle\n",
      "4473.pickle\n",
      "4474.pickle\n",
      "4475.pickle\n",
      "4476.pickle\n",
      "4477.pickle\n",
      "4478.pickle\n",
      "4479.pickle\n",
      "4480.pickle\n",
      "4481.pickle\n",
      "4482.pickle\n",
      "4483.pickle\n",
      "4484.pickle\n",
      "4485.pickle\n",
      "4486.pickle\n",
      "4487.pickle\n",
      "4488.pickle\n",
      "4489.pickle\n",
      "4490.pickle\n",
      "4491.pickle\n",
      "4492.pickle\n",
      "4493.pickle\n",
      "4494.pickle\n",
      "4495.pickle\n",
      "4496.pickle\n",
      "4497.pickle\n",
      "4498.pickle\n",
      "4499.pickle\n",
      "4500.pickle\n",
      "4501.pickle\n",
      "4502.pickle\n",
      "4503.pickle\n",
      "4504.pickle\n",
      "4505.pickle\n",
      "4506.pickle\n",
      "4507.pickle\n",
      "4508.pickle\n",
      "4509.pickle\n",
      "4510.pickle\n",
      "4511.pickle\n",
      "4512.pickle\n",
      "4513.pickle\n",
      "4514.pickle\n",
      "4515.pickle\n",
      "4516.pickle\n",
      "4517.pickle\n",
      "4518.pickle\n",
      "4519.pickle\n",
      "4520.pickle\n",
      "4521.pickle\n",
      "4522.pickle\n",
      "4523.pickle\n",
      "4524.pickle\n",
      "4525.pickle\n",
      "4526.pickle\n",
      "4527.pickle\n",
      "4528.pickle\n",
      "4529.pickle\n",
      "4530.pickle\n",
      "4531.pickle\n",
      "4532.pickle\n",
      "4533.pickle\n",
      "4534.pickle\n",
      "4535.pickle\n",
      "4536.pickle\n",
      "4537.pickle\n",
      "4538.pickle\n",
      "4539.pickle\n",
      "4540.pickle\n",
      "4541.pickle\n",
      "4542.pickle\n",
      "4543.pickle\n",
      "4544.pickle\n",
      "4545.pickle\n",
      "4546.pickle\n",
      "4547.pickle\n",
      "4548.pickle\n",
      "4549.pickle\n",
      "4550.pickle\n",
      "4551.pickle\n",
      "4552.pickle\n",
      "4553.pickle\n",
      "4554.pickle\n",
      "4555.pickle\n",
      "4556.pickle\n",
      "4557.pickle\n",
      "4558.pickle\n",
      "4559.pickle\n",
      "4560.pickle\n",
      "4561.pickle\n",
      "4562.pickle\n",
      "4563.pickle\n",
      "4564.pickle\n",
      "4565.pickle\n",
      "4566.pickle\n",
      "4567.pickle\n",
      "4568.pickle\n",
      "4569.pickle\n",
      "4570.pickle\n",
      "4571.pickle\n",
      "4572.pickle\n",
      "4573.pickle\n",
      "4574.pickle\n",
      "4575.pickle\n",
      "4576.pickle\n",
      "4577.pickle\n",
      "4578.pickle\n",
      "4579.pickle\n",
      "4580.pickle\n",
      "4581.pickle\n",
      "4582.pickle\n",
      "4583.pickle\n",
      "4584.pickle\n",
      "4585.pickle\n",
      "4586.pickle\n",
      "4587.pickle\n",
      "4588.pickle\n",
      "4589.pickle\n",
      "4590.pickle\n",
      "4591.pickle\n",
      "4592.pickle\n",
      "4593.pickle\n",
      "4594.pickle\n",
      "4595.pickle\n",
      "4596.pickle\n",
      "4597.pickle\n",
      "4598.pickle\n",
      "4599.pickle\n",
      "4600.pickle\n",
      "4601.pickle\n",
      "4602.pickle\n",
      "4603.pickle\n",
      "4604.pickle\n",
      "4605.pickle\n",
      "4606.pickle\n",
      "4607.pickle\n",
      "4608.pickle\n",
      "4609.pickle\n",
      "4610.pickle\n",
      "4611.pickle\n",
      "4612.pickle\n",
      "4613.pickle\n",
      "4614.pickle\n",
      "4615.pickle\n",
      "4616.pickle\n",
      "4617.pickle\n",
      "4618.pickle\n",
      "4619.pickle\n",
      "4620.pickle\n",
      "4621.pickle\n",
      "4622.pickle\n",
      "4623.pickle\n",
      "4624.pickle\n",
      "4625.pickle\n",
      "4626.pickle\n",
      "4627.pickle\n",
      "4628.pickle\n",
      "4629.pickle\n",
      "4630.pickle\n",
      "4631.pickle\n",
      "4632.pickle\n",
      "4633.pickle\n",
      "4634.pickle\n",
      "4635.pickle\n",
      "4636.pickle\n",
      "4637.pickle\n",
      "4638.pickle\n",
      "4639.pickle\n",
      "4640.pickle\n",
      "4641.pickle\n",
      "4642.pickle\n",
      "4643.pickle\n",
      "4644.pickle\n",
      "4645.pickle\n",
      "4646.pickle\n",
      "4647.pickle\n",
      "4648.pickle\n",
      "4649.pickle\n",
      "4650.pickle\n",
      "4651.pickle\n",
      "4652.pickle\n",
      "4653.pickle\n",
      "4654.pickle\n",
      "4655.pickle\n",
      "4656.pickle\n",
      "4657.pickle\n",
      "4658.pickle\n",
      "4659.pickle\n",
      "4660.pickle\n",
      "4661.pickle\n",
      "4662.pickle\n",
      "4663.pickle\n",
      "4664.pickle\n",
      "4665.pickle\n",
      "4666.pickle\n",
      "4667.pickle\n",
      "4668.pickle\n",
      "4669.pickle\n",
      "4670.pickle\n",
      "4671.pickle\n",
      "4672.pickle\n",
      "4673.pickle\n",
      "4674.pickle\n",
      "4675.pickle\n",
      "4676.pickle\n",
      "4677.pickle\n",
      "4678.pickle\n",
      "4679.pickle\n",
      "4680.pickle\n",
      "4681.pickle\n",
      "4682.pickle\n",
      "4683.pickle\n",
      "4684.pickle\n",
      "4685.pickle\n",
      "4686.pickle\n",
      "4687.pickle\n",
      "4688.pickle\n",
      "4689.pickle\n",
      "4690.pickle\n",
      "4691.pickle\n",
      "4692.pickle\n",
      "4693.pickle\n",
      "4694.pickle\n",
      "4695.pickle\n",
      "4696.pickle\n",
      "4697.pickle\n",
      "4698.pickle\n",
      "4699.pickle\n",
      "4700.pickle\n",
      "4701.pickle\n",
      "4702.pickle\n",
      "4703.pickle\n",
      "4704.pickle\n",
      "4705.pickle\n",
      "4706.pickle\n",
      "4707.pickle\n",
      "4708.pickle\n",
      "4709.pickle\n",
      "4710.pickle\n",
      "4711.pickle\n",
      "4712.pickle\n",
      "4713.pickle\n",
      "4714.pickle\n",
      "4715.pickle\n",
      "4716.pickle\n",
      "4717.pickle\n",
      "4718.pickle\n",
      "4719.pickle\n",
      "4720.pickle\n",
      "4721.pickle\n",
      "4722.pickle\n",
      "4723.pickle\n",
      "4724.pickle\n",
      "4725.pickle\n",
      "4726.pickle\n",
      "4727.pickle\n",
      "4728.pickle\n",
      "4729.pickle\n",
      "4730.pickle\n",
      "4731.pickle\n",
      "4732.pickle\n",
      "4733.pickle\n",
      "4734.pickle\n",
      "4735.pickle\n",
      "4736.pickle\n",
      "4737.pickle\n",
      "4738.pickle\n",
      "4739.pickle\n",
      "4740.pickle\n",
      "4741.pickle\n",
      "4742.pickle\n",
      "4743.pickle\n",
      "4744.pickle\n",
      "4745.pickle\n",
      "4746.pickle\n",
      "4747.pickle\n",
      "4748.pickle\n",
      "4749.pickle\n",
      "4750.pickle\n",
      "4751.pickle\n",
      "4752.pickle\n",
      "4753.pickle\n",
      "4754.pickle\n",
      "4755.pickle\n",
      "4756.pickle\n",
      "4757.pickle\n",
      "4758.pickle\n",
      "4759.pickle\n",
      "4760.pickle\n",
      "4761.pickle\n",
      "4762.pickle\n",
      "4763.pickle\n",
      "4764.pickle\n",
      "4765.pickle\n",
      "4766.pickle\n",
      "4767.pickle\n",
      "4768.pickle\n",
      "4769.pickle\n",
      "4770.pickle\n",
      "4771.pickle\n",
      "4772.pickle\n",
      "4773.pickle\n",
      "4774.pickle\n",
      "4775.pickle\n",
      "4776.pickle\n",
      "4777.pickle\n",
      "4778.pickle\n",
      "4779.pickle\n",
      "4780.pickle\n",
      "4781.pickle\n",
      "4782.pickle\n",
      "4783.pickle\n",
      "4784.pickle\n",
      "4785.pickle\n",
      "4786.pickle\n",
      "4787.pickle\n",
      "4788.pickle\n",
      "4789.pickle\n",
      "4790.pickle\n",
      "4791.pickle\n",
      "4792.pickle\n",
      "4793.pickle\n",
      "4794.pickle\n",
      "4795.pickle\n",
      "4796.pickle\n",
      "4797.pickle\n",
      "4798.pickle\n",
      "4799.pickle\n",
      "4800.pickle\n",
      "4801.pickle\n",
      "4802.pickle\n",
      "4803.pickle\n",
      "4804.pickle\n",
      "4805.pickle\n",
      "4806.pickle\n",
      "4807.pickle\n",
      "4808.pickle\n",
      "4809.pickle\n",
      "4810.pickle\n",
      "4811.pickle\n",
      "4812.pickle\n",
      "4813.pickle\n",
      "4814.pickle\n",
      "4815.pickle\n",
      "4816.pickle\n",
      "4817.pickle\n",
      "4818.pickle\n",
      "4819.pickle\n",
      "4820.pickle\n",
      "4821.pickle\n",
      "4822.pickle\n",
      "4823.pickle\n",
      "4824.pickle\n",
      "4825.pickle\n",
      "4826.pickle\n",
      "4827.pickle\n",
      "4828.pickle\n",
      "4829.pickle\n",
      "4830.pickle\n",
      "4831.pickle\n",
      "4832.pickle\n",
      "4833.pickle\n",
      "4834.pickle\n",
      "4835.pickle\n",
      "4836.pickle\n",
      "4837.pickle\n",
      "4838.pickle\n",
      "4839.pickle\n",
      "4840.pickle\n",
      "4841.pickle\n",
      "4842.pickle\n",
      "4843.pickle\n",
      "4844.pickle\n",
      "4845.pickle\n",
      "4846.pickle\n",
      "4847.pickle\n",
      "4848.pickle\n",
      "4849.pickle\n",
      "4850.pickle\n",
      "4851.pickle\n",
      "4852.pickle\n",
      "4853.pickle\n",
      "4854.pickle\n",
      "4855.pickle\n",
      "4856.pickle\n",
      "4857.pickle\n",
      "4858.pickle\n",
      "4859.pickle\n",
      "4860.pickle\n",
      "4861.pickle\n",
      "4862.pickle\n",
      "4863.pickle\n",
      "4864.pickle\n",
      "4865.pickle\n",
      "4866.pickle\n",
      "4867.pickle\n",
      "4868.pickle\n",
      "4869.pickle\n",
      "4870.pickle\n",
      "4871.pickle\n",
      "4872.pickle\n",
      "4873.pickle\n",
      "4874.pickle\n",
      "4875.pickle\n",
      "4876.pickle\n",
      "4877.pickle\n",
      "4878.pickle\n",
      "4879.pickle\n",
      "4880.pickle\n",
      "4881.pickle\n",
      "4882.pickle\n",
      "4883.pickle\n",
      "4884.pickle\n",
      "4885.pickle\n",
      "4886.pickle\n",
      "4887.pickle\n",
      "4888.pickle\n",
      "4889.pickle\n",
      "4890.pickle\n",
      "4891.pickle\n",
      "4892.pickle\n",
      "4893.pickle\n",
      "4894.pickle\n",
      "4895.pickle\n",
      "4896.pickle\n",
      "4897.pickle\n",
      "4898.pickle\n",
      "4899.pickle\n",
      "4900.pickle\n",
      "4901.pickle\n",
      "4902.pickle\n",
      "4903.pickle\n",
      "4904.pickle\n",
      "4905.pickle\n",
      "4906.pickle\n",
      "4907.pickle\n",
      "4908.pickle\n",
      "4909.pickle\n",
      "4910.pickle\n",
      "4911.pickle\n",
      "4912.pickle\n",
      "4913.pickle\n",
      "4914.pickle\n",
      "4915.pickle\n",
      "4916.pickle\n",
      "4917.pickle\n",
      "4918.pickle\n",
      "4919.pickle\n",
      "4920.pickle\n",
      "4921.pickle\n",
      "4922.pickle\n",
      "4923.pickle\n",
      "4924.pickle\n",
      "4925.pickle\n",
      "4926.pickle\n",
      "4927.pickle\n",
      "4928.pickle\n",
      "4929.pickle\n",
      "4930.pickle\n",
      "4931.pickle\n",
      "4932.pickle\n",
      "4933.pickle\n",
      "4934.pickle\n",
      "4935.pickle\n",
      "4936.pickle\n",
      "4937.pickle\n",
      "4938.pickle\n",
      "4939.pickle\n",
      "4940.pickle\n",
      "4941.pickle\n",
      "4942.pickle\n",
      "4943.pickle\n",
      "4944.pickle\n",
      "4945.pickle\n",
      "4946.pickle\n",
      "4947.pickle\n",
      "4948.pickle\n",
      "4949.pickle\n",
      "4950.pickle\n",
      "4951.pickle\n",
      "4952.pickle\n",
      "4953.pickle\n",
      "4954.pickle\n",
      "4955.pickle\n",
      "4956.pickle\n",
      "4957.pickle\n",
      "4958.pickle\n",
      "4959.pickle\n",
      "4960.pickle\n",
      "4961.pickle\n",
      "4962.pickle\n",
      "4963.pickle\n",
      "4964.pickle\n",
      "4965.pickle\n",
      "4966.pickle\n",
      "4967.pickle\n",
      "4968.pickle\n",
      "4969.pickle\n",
      "4970.pickle\n",
      "4971.pickle\n",
      "4972.pickle\n",
      "4973.pickle\n",
      "4974.pickle\n",
      "4975.pickle\n",
      "4976.pickle\n",
      "4977.pickle\n",
      "4978.pickle\n",
      "4979.pickle\n",
      "4980.pickle\n",
      "4981.pickle\n",
      "4982.pickle\n",
      "4983.pickle\n",
      "4984.pickle\n",
      "4985.pickle\n",
      "4986.pickle\n",
      "4987.pickle\n",
      "4988.pickle\n",
      "4989.pickle\n",
      "4990.pickle\n",
      "4991.pickle\n",
      "4992.pickle\n",
      "4993.pickle\n",
      "4994.pickle\n",
      "4995.pickle\n",
      "4996.pickle\n",
      "4997.pickle\n",
      "4998.pickle\n",
      "4999.pickle\n",
      "5000.pickle\n",
      "5001.pickle\n",
      "5002.pickle\n",
      "5003.pickle\n",
      "5004.pickle\n",
      "5005.pickle\n",
      "5006.pickle\n",
      "5007.pickle\n",
      "5008.pickle\n",
      "5009.pickle\n",
      "5010.pickle\n",
      "5011.pickle\n",
      "5012.pickle\n",
      "5013.pickle\n",
      "5014.pickle\n",
      "5015.pickle\n",
      "5016.pickle\n",
      "5017.pickle\n",
      "5018.pickle\n",
      "5019.pickle\n",
      "5020.pickle\n",
      "5021.pickle\n",
      "5022.pickle\n",
      "5023.pickle\n",
      "5024.pickle\n",
      "5025.pickle\n",
      "5026.pickle\n",
      "5027.pickle\n",
      "5028.pickle\n",
      "5029.pickle\n",
      "5030.pickle\n",
      "5031.pickle\n",
      "5032.pickle\n",
      "5033.pickle\n",
      "5034.pickle\n",
      "5035.pickle\n",
      "5036.pickle\n",
      "5037.pickle\n",
      "5038.pickle\n",
      "5039.pickle\n",
      "5040.pickle\n",
      "5041.pickle\n",
      "5042.pickle\n",
      "5043.pickle\n",
      "5044.pickle\n",
      "5045.pickle\n",
      "5046.pickle\n",
      "5047.pickle\n",
      "5048.pickle\n",
      "5049.pickle\n",
      "5050.pickle\n",
      "5051.pickle\n",
      "5052.pickle\n",
      "5053.pickle\n",
      "5054.pickle\n",
      "5055.pickle\n",
      "5056.pickle\n",
      "5057.pickle\n",
      "5058.pickle\n",
      "5059.pickle\n",
      "5060.pickle\n",
      "5061.pickle\n",
      "5062.pickle\n",
      "5063.pickle\n",
      "5064.pickle\n",
      "5065.pickle\n",
      "5066.pickle\n",
      "5067.pickle\n",
      "5068.pickle\n",
      "5069.pickle\n",
      "5070.pickle\n",
      "5071.pickle\n",
      "5072.pickle\n",
      "5073.pickle\n",
      "5074.pickle\n",
      "5075.pickle\n",
      "5076.pickle\n",
      "5077.pickle\n",
      "5078.pickle\n",
      "5079.pickle\n",
      "5080.pickle\n",
      "5081.pickle\n",
      "5082.pickle\n",
      "5083.pickle\n",
      "5084.pickle\n",
      "5085.pickle\n",
      "5086.pickle\n",
      "5087.pickle\n",
      "5088.pickle\n",
      "5089.pickle\n",
      "5090.pickle\n",
      "5091.pickle\n",
      "5092.pickle\n",
      "5093.pickle\n",
      "5094.pickle\n",
      "5095.pickle\n",
      "5096.pickle\n",
      "5097.pickle\n",
      "5098.pickle\n",
      "5099.pickle\n",
      "5100.pickle\n",
      "5101.pickle\n",
      "5102.pickle\n",
      "5103.pickle\n",
      "5104.pickle\n",
      "5105.pickle\n",
      "5106.pickle\n",
      "5107.pickle\n",
      "5108.pickle\n",
      "5109.pickle\n",
      "5110.pickle\n",
      "5111.pickle\n",
      "5112.pickle\n",
      "5113.pickle\n",
      "5114.pickle\n",
      "5115.pickle\n",
      "5116.pickle\n",
      "5117.pickle\n",
      "5118.pickle\n",
      "5119.pickle\n",
      "5120.pickle\n",
      "5121.pickle\n",
      "5122.pickle\n",
      "5123.pickle\n",
      "5124.pickle\n",
      "5125.pickle\n",
      "5126.pickle\n",
      "5127.pickle\n",
      "5128.pickle\n",
      "5129.pickle\n",
      "5130.pickle\n",
      "5131.pickle\n",
      "5132.pickle\n",
      "5133.pickle\n",
      "5134.pickle\n",
      "5135.pickle\n",
      "5136.pickle\n",
      "5137.pickle\n",
      "5138.pickle\n",
      "5139.pickle\n",
      "5140.pickle\n",
      "5141.pickle\n",
      "5142.pickle\n",
      "5143.pickle\n",
      "5144.pickle\n",
      "5145.pickle\n",
      "5146.pickle\n",
      "5147.pickle\n",
      "5148.pickle\n",
      "5149.pickle\n",
      "5150.pickle\n",
      "5151.pickle\n",
      "5152.pickle\n",
      "5153.pickle\n",
      "5154.pickle\n",
      "5155.pickle\n",
      "5156.pickle\n",
      "5157.pickle\n",
      "5158.pickle\n",
      "5159.pickle\n",
      "5160.pickle\n",
      "5161.pickle\n",
      "5162.pickle\n",
      "5163.pickle\n",
      "5164.pickle\n",
      "5165.pickle\n",
      "5166.pickle\n",
      "5167.pickle\n",
      "5168.pickle\n",
      "5169.pickle\n",
      "5170.pickle\n",
      "5171.pickle\n",
      "5172.pickle\n",
      "5173.pickle\n",
      "5174.pickle\n",
      "5175.pickle\n",
      "5176.pickle\n",
      "5177.pickle\n",
      "5178.pickle\n",
      "5179.pickle\n",
      "5180.pickle\n",
      "5181.pickle\n",
      "5182.pickle\n",
      "5183.pickle\n",
      "5184.pickle\n",
      "5185.pickle\n",
      "5186.pickle\n",
      "5187.pickle\n",
      "5188.pickle\n",
      "5189.pickle\n",
      "5190.pickle\n",
      "5191.pickle\n",
      "5192.pickle\n",
      "5193.pickle\n",
      "5194.pickle\n",
      "5195.pickle\n",
      "5196.pickle\n",
      "5197.pickle\n",
      "5198.pickle\n",
      "5199.pickle\n",
      "5200.pickle\n",
      "5201.pickle\n",
      "5202.pickle\n",
      "5203.pickle\n",
      "5204.pickle\n",
      "5205.pickle\n",
      "5206.pickle\n",
      "5207.pickle\n",
      "5208.pickle\n",
      "5209.pickle\n",
      "5210.pickle\n",
      "5211.pickle\n",
      "5212.pickle\n",
      "5213.pickle\n",
      "5214.pickle\n",
      "5215.pickle\n",
      "5216.pickle\n",
      "5217.pickle\n",
      "5218.pickle\n",
      "5219.pickle\n",
      "5220.pickle\n",
      "5221.pickle\n",
      "5222.pickle\n",
      "5223.pickle\n",
      "5224.pickle\n",
      "5225.pickle\n",
      "5226.pickle\n",
      "5227.pickle\n",
      "5228.pickle\n",
      "5229.pickle\n",
      "5230.pickle\n",
      "5231.pickle\n",
      "5232.pickle\n",
      "5233.pickle\n",
      "5234.pickle\n",
      "5235.pickle\n",
      "5236.pickle\n",
      "5237.pickle\n",
      "5238.pickle\n",
      "5239.pickle\n",
      "5240.pickle\n",
      "5241.pickle\n",
      "5242.pickle\n",
      "5243.pickle\n",
      "5244.pickle\n",
      "5245.pickle\n",
      "5246.pickle\n",
      "5247.pickle\n",
      "5248.pickle\n",
      "5249.pickle\n",
      "5250.pickle\n",
      "5251.pickle\n",
      "5252.pickle\n",
      "5253.pickle\n",
      "5254.pickle\n",
      "5255.pickle\n",
      "5256.pickle\n",
      "5257.pickle\n",
      "5258.pickle\n",
      "5259.pickle\n",
      "5260.pickle\n",
      "5261.pickle\n",
      "5262.pickle\n",
      "5263.pickle\n",
      "5264.pickle\n",
      "5265.pickle\n",
      "5266.pickle\n",
      "5267.pickle\n",
      "5268.pickle\n",
      "5269.pickle\n",
      "5270.pickle\n",
      "5271.pickle\n",
      "5272.pickle\n",
      "5273.pickle\n",
      "5274.pickle\n",
      "5275.pickle\n",
      "5276.pickle\n",
      "5277.pickle\n",
      "5278.pickle\n",
      "5279.pickle\n",
      "5280.pickle\n",
      "5281.pickle\n",
      "5282.pickle\n",
      "5283.pickle\n",
      "5284.pickle\n",
      "5285.pickle\n",
      "5286.pickle\n",
      "5287.pickle\n",
      "5288.pickle\n",
      "5289.pickle\n",
      "5290.pickle\n",
      "5291.pickle\n",
      "5292.pickle\n",
      "5293.pickle\n",
      "5294.pickle\n",
      "5295.pickle\n",
      "5296.pickle\n",
      "5297.pickle\n",
      "5298.pickle\n",
      "5299.pickle\n",
      "5300.pickle\n",
      "5301.pickle\n",
      "5302.pickle\n",
      "5303.pickle\n",
      "5304.pickle\n",
      "5305.pickle\n",
      "5306.pickle\n",
      "5307.pickle\n",
      "5308.pickle\n",
      "5309.pickle\n",
      "5310.pickle\n",
      "5311.pickle\n",
      "5312.pickle\n",
      "5313.pickle\n",
      "5314.pickle\n",
      "5315.pickle\n",
      "5316.pickle\n",
      "5317.pickle\n",
      "5318.pickle\n",
      "5319.pickle\n",
      "5320.pickle\n",
      "5321.pickle\n",
      "5322.pickle\n",
      "5323.pickle\n",
      "5324.pickle\n",
      "5325.pickle\n",
      "5326.pickle\n",
      "5327.pickle\n",
      "5328.pickle\n",
      "5329.pickle\n",
      "5330.pickle\n",
      "5331.pickle\n",
      "5332.pickle\n",
      "5333.pickle\n",
      "5334.pickle\n",
      "5335.pickle\n",
      "5336.pickle\n",
      "5337.pickle\n",
      "5338.pickle\n",
      "5339.pickle\n",
      "5340.pickle\n",
      "5341.pickle\n",
      "5342.pickle\n",
      "5343.pickle\n",
      "5344.pickle\n",
      "5345.pickle\n",
      "5346.pickle\n",
      "5347.pickle\n",
      "5348.pickle\n",
      "5349.pickle\n",
      "5350.pickle\n",
      "5351.pickle\n",
      "5352.pickle\n",
      "5353.pickle\n",
      "5354.pickle\n",
      "5355.pickle\n",
      "5356.pickle\n",
      "5357.pickle\n",
      "5358.pickle\n",
      "5359.pickle\n",
      "5360.pickle\n",
      "5361.pickle\n",
      "5362.pickle\n",
      "5363.pickle\n",
      "5364.pickle\n",
      "5365.pickle\n",
      "5366.pickle\n",
      "5367.pickle\n",
      "5368.pickle\n",
      "5369.pickle\n",
      "5370.pickle\n",
      "5371.pickle\n",
      "5372.pickle\n",
      "5373.pickle\n",
      "5374.pickle\n",
      "5375.pickle\n",
      "5376.pickle\n",
      "5377.pickle\n",
      "5378.pickle\n",
      "5379.pickle\n",
      "5380.pickle\n",
      "5381.pickle\n",
      "5382.pickle\n",
      "5383.pickle\n",
      "5384.pickle\n",
      "5385.pickle\n",
      "5386.pickle\n",
      "5387.pickle\n",
      "5388.pickle\n",
      "5389.pickle\n",
      "5390.pickle\n",
      "5391.pickle\n",
      "5392.pickle\n",
      "5393.pickle\n",
      "5394.pickle\n",
      "5395.pickle\n",
      "5396.pickle\n",
      "5397.pickle\n",
      "5398.pickle\n",
      "5399.pickle\n",
      "5400.pickle\n",
      "5401.pickle\n",
      "5402.pickle\n",
      "5403.pickle\n",
      "5404.pickle\n",
      "5405.pickle\n",
      "5406.pickle\n",
      "5407.pickle\n",
      "5408.pickle\n",
      "5409.pickle\n",
      "5410.pickle\n",
      "5411.pickle\n",
      "5412.pickle\n",
      "5413.pickle\n",
      "5414.pickle\n",
      "5415.pickle\n",
      "5416.pickle\n",
      "5417.pickle\n",
      "5418.pickle\n",
      "5419.pickle\n",
      "5420.pickle\n",
      "5421.pickle\n",
      "5422.pickle\n",
      "5423.pickle\n",
      "5424.pickle\n",
      "5425.pickle\n",
      "5426.pickle\n",
      "5427.pickle\n",
      "5428.pickle\n",
      "5429.pickle\n",
      "5430.pickle\n",
      "5431.pickle\n",
      "5432.pickle\n",
      "5433.pickle\n",
      "5434.pickle\n",
      "5435.pickle\n",
      "5436.pickle\n",
      "5437.pickle\n",
      "5438.pickle\n",
      "5439.pickle\n",
      "5440.pickle\n",
      "5441.pickle\n",
      "5442.pickle\n",
      "5443.pickle\n",
      "5444.pickle\n",
      "5445.pickle\n",
      "5446.pickle\n",
      "5447.pickle\n",
      "5448.pickle\n",
      "5449.pickle\n",
      "5450.pickle\n",
      "5451.pickle\n",
      "5452.pickle\n",
      "5453.pickle\n",
      "5454.pickle\n",
      "5455.pickle\n",
      "5456.pickle\n",
      "5457.pickle\n",
      "5458.pickle\n",
      "5459.pickle\n",
      "5460.pickle\n",
      "5461.pickle\n",
      "5462.pickle\n",
      "5463.pickle\n",
      "5464.pickle\n",
      "5465.pickle\n",
      "5466.pickle\n",
      "5467.pickle\n",
      "5468.pickle\n",
      "5469.pickle\n",
      "5470.pickle\n",
      "5471.pickle\n",
      "5472.pickle\n",
      "5473.pickle\n",
      "5474.pickle\n",
      "5475.pickle\n",
      "5476.pickle\n",
      "5477.pickle\n",
      "5478.pickle\n",
      "5479.pickle\n",
      "5480.pickle\n",
      "5481.pickle\n",
      "5482.pickle\n",
      "5483.pickle\n",
      "5484.pickle\n",
      "5485.pickle\n",
      "5486.pickle\n",
      "5487.pickle\n",
      "5488.pickle\n",
      "5489.pickle\n",
      "5490.pickle\n",
      "5491.pickle\n",
      "5492.pickle\n",
      "5493.pickle\n",
      "5494.pickle\n",
      "5495.pickle\n",
      "5496.pickle\n",
      "5497.pickle\n",
      "5498.pickle\n",
      "5499.pickle\n",
      "5500.pickle\n",
      "5501.pickle\n",
      "5502.pickle\n",
      "5503.pickle\n",
      "5504.pickle\n",
      "5505.pickle\n",
      "5506.pickle\n",
      "5507.pickle\n",
      "5508.pickle\n",
      "5509.pickle\n",
      "5510.pickle\n",
      "5511.pickle\n",
      "5512.pickle\n",
      "5513.pickle\n",
      "5514.pickle\n",
      "5515.pickle\n",
      "5516.pickle\n",
      "5517.pickle\n",
      "5518.pickle\n",
      "5519.pickle\n",
      "5520.pickle\n",
      "5521.pickle\n",
      "5522.pickle\n",
      "5523.pickle\n",
      "5524.pickle\n",
      "5525.pickle\n",
      "5526.pickle\n",
      "5527.pickle\n",
      "5528.pickle\n",
      "5529.pickle\n",
      "5530.pickle\n",
      "5531.pickle\n",
      "5532.pickle\n",
      "5533.pickle\n",
      "5534.pickle\n",
      "5535.pickle\n",
      "5536.pickle\n",
      "5537.pickle\n",
      "5538.pickle\n",
      "5539.pickle\n",
      "5540.pickle\n",
      "5541.pickle\n",
      "5542.pickle\n",
      "5543.pickle\n",
      "5544.pickle\n",
      "5545.pickle\n",
      "5546.pickle\n",
      "5547.pickle\n",
      "5548.pickle\n",
      "5549.pickle\n",
      "5550.pickle\n",
      "5551.pickle\n",
      "5552.pickle\n",
      "5553.pickle\n",
      "5554.pickle\n",
      "5555.pickle\n",
      "5556.pickle\n"
     ]
    }
   ],
   "source": [
    "def inference(model, data_test, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in DataLoader(data_test, batch_size=1):\n",
    "            data = data.to(device)\n",
    "            pred = model(data)\n",
    "\n",
    "            pred = pred.cpu().numpy()\n",
    "\n",
    "            file_name = data.file_name\n",
    "\n",
    "            assert len(file_name) == 1\n",
    "\n",
    "            yield file_name[0], pred\n",
    "\n",
    "inv_room_mapping = {val: key for key, val in constants.ROOM_MAPPING.items()}\n",
    "\n",
    "for file_name, pred in inference(model, ds_test, device=\"cpu\"):\n",
    "    graph_nx = load_pickle(os.path.join(ds_test.graph_path, file_name))\n",
    "\n",
    "    for node in graph_nx.nodes:\n",
    "        graph_nx.nodes[node]['n_corners'] = pred[node]\n",
    "\n",
    "    # print(graph_nx.nodes(data=True))\n",
    "\n",
    "    print(file_name)\n",
    "    \n",
    "    save_pickle(graph_nx, os.path.join(pred_graph_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.graph_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATModel(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(4, 32, heads=1)\n",
       "    (1): GATConv(32, 32, heads=1)\n",
       "  )\n",
       "  (linear): Linear(in_features=32, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchhousediff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
